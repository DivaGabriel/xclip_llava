{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d79346f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vlm/anaconda3/envs/videollava/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/vlm/anaconda3/envs/videollava/lib/python3.10/site-packages/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.\n",
      "  warnings.warn(\n",
      "/home/vlm/anaconda3/envs/videollava/lib/python3.10/site-packages/torchvision/transforms/_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.\n",
      "  warnings.warn(\n",
      "/home/vlm/anaconda3/envs/videollava/lib/python3.10/site-packages/torchvision/transforms/functional_tensor.py:5: UserWarning: The torchvision.transforms.functional_tensor module is deprecated in 0.15 and will be **removed in 0.17**. Please don't rely on it. You probably just need to use APIs in torchvision.transforms.functional or in torchvision.transforms.v2.functional.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import av\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, AutoModel, XCLIPVisionModel\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import types\n",
    "np.random.seed(0)\n",
    "from torchvision import transforms\n",
    "from videollava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN\n",
    "from videollava.conversation import conv_templates, SeparatorStyle\n",
    "from videollava.model.builder import load_pretrained_model\n",
    "from videollava.utils import disable_torch_init\n",
    "from videollava.mm_utils import tokenizer_image_token, get_model_name_from_path, KeywordsStoppingCriteria\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81f5399d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vlm/anaconda3/envs/videollava/lib/python3.10/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "processor = AutoProcessor.from_pretrained(\"microsoft/xclip-base-patch32\")\n",
    "model = AutoModel.from_pretrained(\"microsoft/xclip-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "860e163b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`id2label` is found in both `text_config_dict` and `text_config` but with different values. The value `text_config_dict[\"id2label\"]` will be used instead.\n"
     ]
    }
   ],
   "source": [
    "XCLIP_processor = AutoProcessor.from_pretrained(\"microsoft/xclip-large-patch14-16-frames\")\n",
    "XCLIP_model = AutoModel.from_pretrained(\"microsoft/xclip-large-patch14-16-frames\").to(\"cuda\").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cf7f7788",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_4bit, load_8bit = True, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24026629",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "del XCLIP_processor,XCLIP_model\n",
    "gc.collect()                   # 釋放 Python 層級的物件引用\n",
    "torch.cuda.empty_cache()      # 清除 PyTorch 的 GPU 快取記憶體"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd9b7f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = \"pts-daily-20241114-224-1.mp4\"\n",
    "text = [\n",
    "    \"關心昨天晚間6點36分，花蓮富里鄉、發生芮氏規模5.8地震，幾乎全台有感，所幸都沒有傳出嚴重災情，氣象署地震測報中心不排除今明後三天，還可能會有規模4.5到5.5的餘震，大家多加留意＃。\",\n",
    "    \"關心早上地牛翻身，07:05花蓮外海發生規模6.2的有感地震，震度3級以上地區，包括有宜蘭、花蓮、新北、新竹、嘉義、雲林、彰化。更多地震消息，請鎖定公視各節新聞。\",\n",
    "    \"嘉義縣新港鄉昨天下午五點半，發生芮氏規模5.5地震，深度8.5公里，地震明顯，使得高鐵兩班列車暫時停車，嘉義民雄鄉有民宅圍牆被震倒，還有雲林古坑鄉149甲線內湖明隧道則發生坍方落石，所幸無人受傷。\",\n",
    "    \"兔年春節10天連假已經結束，今天開始上班，不少人早上起不來，今年過年冷颼颼，寒流發威，昨天清晨苗栗頭屋出現4度低溫，是入冬以來最低氣溫紀錄。光是1月27日到1月28日二天，全台超過140人，疑似因天冷猝死。氣象局指示，今天寒流雖然開始減弱，不過，明後天，各地都還是日夜溫差大，大家出門要做好保暖工作。\",\n",
    "    \"關心地震消息\", \n",
    "    \"今天晚餐吃什麼\", \n",
    "    \"歡迎收看公視新聞\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f788f681",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_video_pyav(container, indices):\n",
    "    '''\n",
    "    Decode the video with PyAV decoder.\n",
    "    Args:\n",
    "        container (`av.container.input.InputContainer`): PyAV container.\n",
    "        indices (`List[int]`): List of frame indices to decode.\n",
    "    Returns:\n",
    "        result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).\n",
    "    '''\n",
    "    frames = []\n",
    "    container.seek(0)\n",
    "    start_index = indices[0]\n",
    "    end_index = indices[-1]\n",
    "    for i, frame in enumerate(container.decode(video=0)):\n",
    "        if i > end_index:\n",
    "            break\n",
    "        if i >= start_index and i in indices:\n",
    "            frames.append(frame)\n",
    "    return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n",
    "def sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n",
    "    '''\n",
    "    Sample a given number of frame indices from the video.\n",
    "    Args:\n",
    "        clip_len (`int`): Total number of frames to sample.\n",
    "        frame_sample_rate (`int`): Sample every n-th frame.\n",
    "        seg_len (`int`): Maximum allowed index of sample's last frame.\n",
    "    Returns:\n",
    "        indices (`List[int]`): List of sampled frame indices\n",
    "    '''\n",
    "    converted_len = int(clip_len * frame_sample_rate)\n",
    "    end_idx = np.random.randint(converted_len, seg_len)\n",
    "    start_idx = end_idx - converted_len\n",
    "    indices = np.linspace(start_idx, end_idx, num=clip_len)\n",
    "    indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n",
    "    return indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd9ff51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "container = av.open(video_path)\n",
    "indices = sample_frame_indices(clip_len=16, frame_sample_rate=1, seg_len=container.streams.video[0].frames)\n",
    "video = read_video_pyav(container, indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bf9d0612",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 1080, 1920, 3)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b17de697",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_inputs = XCLIP_processor.image_processor(\n",
    "    images=list(video),\n",
    "    return_tensors=\"pt\",\n",
    "    size={\"height\": 336, \"width\":336},  ######\n",
    "    do_resize=True,\n",
    "    do_center_crop=False  \n",
    ")\n",
    "pixel_values = video_inputs[\"pixel_values\"].to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b49d6d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 3, 336, 336])\n",
      "torch.Size([1, 16, 3, 336, 336])\n",
      "torch.Size([1, 3, 16, 336, 336])\n"
     ]
    }
   ],
   "source": [
    "print(video_inputs[\"pixel_values\"].shape)\n",
    "_,num_frames, num_channels, height, width = pixel_values.shape\n",
    "print(pixel_values.shape)\n",
    "pixel_values = pixel_values.permute(0, 2, 1, 3, 4)\n",
    "print(pixel_values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4572b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 50, 768])\n"
     ]
    }
   ],
   "source": [
    "# 模型搬到 GPU\n",
    "model.vision_model = model.vision_model.to(\"cuda\")\n",
    "with torch.no_grad():\n",
    "    vit_out = model.vision_model(pixel_values=pixel_values)  # ✅ OK\n",
    "    patch_features = vit_out.last_hidden_state  # [8, num_tokens, 768]\n",
    "print(patch_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f167e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 577, 1024])\n"
     ]
    }
   ],
   "source": [
    "# 模型搬到 GPU\n",
    "XCLIP_model.vision_model = XCLIP_model.vision_model.to(\"cuda\")\n",
    "with torch.no_grad():\n",
    "    vit16_out = XCLIP_model.vision_model(pixel_values=pixel_values)  # ✅ OK\n",
    "    patch16_features = vit16_out.last_hidden_state \n",
    "print(patch16_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b29dc80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLaVA開始\n",
      "LLaVA載入模型\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [03:55<00:00, 117.74s/it]\n",
      "Some weights of the model checkpoint at LanguageBind/Video-LLaVA-7B were not used when initializing LlavaLlamaForCausalLM: ['model.video_tower.video_tower.encoder.layers.17.temporal_attn.out_proj.weight', 'model.video_tower.video_tower.encoder.layers.2.layer_norm1.weight', 'model.video_tower.video_tower.encoder.layers.17.mlp.fc2.bias', 'model.video_tower.video_tower.encoder.layers.4.temporal_layer_norm1.weight', 'model.video_tower.video_tower.encoder.layers.5.temporal_attn.v_proj.bias', 'model.video_tower.video_tower.encoder.layers.6.temporal_attn.v_proj.weight', 'model.video_tower.video_tower.encoder.layers.2.temporal_attn.q_proj.bias', 'model.image_tower.image_tower.encoder.layers.13.layer_norm1.bias', 'model.video_tower.video_tower.encoder.layers.23.self_attn.out_proj.bias', 'model.video_tower.video_tower.encoder.layers.6.self_attn.q_proj.bias', 'model.video_tower.video_tower.encoder.layers.2.self_attn.q_proj.weight', 'model.image_tower.image_tower.encoder.layers.18.mlp.fc2.bias', 'model.video_tower.video_tower.encoder.layers.13.temporal_attn.v_proj.weight', 'model.image_tower.image_tower.encoder.layers.22.layer_norm2.bias', 'model.image_tower.image_tower.encoder.layers.13.layer_norm2.bias', 'model.video_tower.video_tower.encoder.layers.20.self_attn.out_proj.bias', 'model.video_tower.video_tower.encoder.layers.22.temporal_layer_norm1.weight', 'model.image_tower.image_tower.encoder.layers.22.self_attn.q_proj.weight', 'model.image_tower.image_tower.encoder.layers.13.self_attn.out_proj.bias', 'model.video_tower.video_tower.encoder.layers.7.temporal_embedding', 'model.video_tower.video_tower.encoder.layers.21.temporal_attn.out_proj.weight', 'model.video_tower.video_tower.encoder.layers.8.self_attn.k_proj.weight', 'model.video_tower.video_tower.encoder.layers.14.mlp.fc2.weight', 'model.video_tower.video_tower.encoder.layers.5.self_attn.k_proj.bias', 'model.video_tower.video_tower.encoder.layers.10.layer_norm1.weight', 'model.video_tower.video_tower.encoder.layers.0.temporal_attn.q_proj.weight', 'model.image_tower.image_tower.encoder.layers.10.layer_norm2.weight', 'model.image_tower.image_tower.encoder.layers.4.mlp.fc2.weight', 'model.video_tower.video_tower.encoder.layers.23.temporal_attn.k_proj.weight', 'model.image_tower.image_tower.encoder.layers.2.layer_norm2.weight', 'model.video_tower.video_tower.encoder.layers.23.mlp.fc1.weight', 'model.video_tower.video_tower.encoder.layers.16.self_attn.q_proj.weight', 'model.video_tower.video_tower.encoder.layers.22.self_attn.v_proj.weight', 'model.image_tower.image_tower.encoder.layers.20.self_attn.k_proj.weight', 'model.video_tower.video_tower.encoder.layers.8.self_attn.v_proj.weight', 'model.video_tower.video_tower.encoder.layers.15.self_attn.v_proj.weight', 'model.video_tower.video_tower.encoder.layers.10.temporal_attn.out_proj.bias', 'model.video_tower.video_tower.encoder.layers.11.mlp.fc2.weight', 'model.video_tower.video_tower.encoder.layers.5.self_attn.v_proj.bias', 'model.video_tower.video_tower.encoder.layers.6.temporal_attn.out_proj.weight', 'model.video_tower.video_tower.encoder.layers.19.mlp.fc1.bias', 'model.video_tower.video_tower.encoder.layers.23.temporal_attn.q_proj.bias', 'model.video_tower.video_tower.encoder.layers.23.temporal_embedding', 'model.video_tower.video_tower.encoder.layers.23.self_attn.v_proj.weight', 'model.image_tower.image_tower.encoder.layers.9.self_attn.q_proj.weight', 'model.video_tower.video_tower.encoder.layers.14.self_attn.out_proj.bias', 'model.image_tower.image_tower.encoder.layers.20.layer_norm2.weight', 'model.image_tower.image_tower.encoder.layers.17.mlp.fc2.bias', 'model.image_tower.image_tower.encoder.layers.2.self_attn.k_proj.bias', 'model.video_tower.video_tower.encoder.layers.6.temporal_layer_norm1.bias', 'model.video_tower.video_tower.encoder.layers.11.layer_norm1.weight', 'model.image_tower.image_tower.encoder.layers.3.self_attn.out_proj.bias', 'model.video_tower.video_tower.encoder.layers.3.mlp.fc1.bias', 'model.video_tower.video_tower.encoder.layers.3.mlp.fc2.bias', 'model.image_tower.image_tower.encoder.layers.21.self_attn.q_proj.bias', 'model.image_tower.image_tower.encoder.layers.17.self_attn.v_proj.weight', 'model.image_tower.image_tower.encoder.layers.7.self_attn.q_proj.bias', 'model.image_tower.image_tower.encoder.layers.13.self_attn.k_proj.bias', 'model.video_tower.video_tower.encoder.layers.13.self_attn.v_proj.weight', 'model.video_tower.video_tower.encoder.layers.9.temporal_layer_norm1.weight', 'model.video_tower.video_tower.encoder.layers.4.layer_norm1.bias', 'model.image_tower.image_tower.encoder.layers.9.layer_norm1.weight', 'model.image_tower.image_tower.encoder.layers.15.layer_norm1.weight', 'model.video_tower.video_tower.encoder.layers.12.self_attn.v_proj.bias', 'model.image_tower.image_tower.encoder.layers.23.mlp.fc2.weight', 'model.video_tower.video_tower.encoder.layers.23.temporal_attn.q_proj.weight', 'model.image_tower.image_tower.encoder.layers.20.self_attn.v_proj.weight', 'model.image_tower.image_tower.encoder.layers.23.layer_norm2.bias', 'model.video_tower.video_tower.encoder.layers.21.temporal_layer_norm1.bias', 'model.image_tower.image_tower.encoder.layers.16.self_attn.q_proj.bias', 'model.video_tower.video_tower.encoder.layers.22.layer_norm1.weight', 'model.video_tower.video_tower.encoder.layers.21.temporal_layer_norm1.weight', 'model.image_tower.image_tower.encoder.layers.10.layer_norm2.bias', 'model.video_tower.video_tower.encoder.layers.0.temporal_attn.q_proj.bias', 'model.video_tower.video_tower.encoder.layers.11.self_attn.k_proj.weight', 'model.image_tower.image_tower.encoder.layers.12.self_attn.v_proj.bias', 'model.video_tower.video_tower.encoder.layers.22.layer_norm1.bias', 'model.image_tower.image_tower.encoder.layers.4.self_attn.out_proj.weight', 'model.image_tower.image_tower.encoder.layers.13.self_attn.q_proj.bias', 'model.video_tower.video_tower.encoder.layers.5.temporal_attn.q_proj.weight', 'model.image_tower.image_tower.encoder.layers.7.self_attn.k_proj.weight', 'model.image_tower.image_tower.encoder.layers.8.self_attn.out_proj.weight', 'model.video_tower.video_tower.encoder.layers.8.temporal_layer_norm1.bias', 'model.image_tower.image_tower.encoder.layers.16.mlp.fc1.weight', 'model.video_tower.video_tower.encoder.layers.23.temporal_attn.out_proj.weight', 'model.video_tower.video_tower.encoder.layers.12.temporal_embedding', 'model.image_tower.image_tower.encoder.layers.3.layer_norm1.weight', 'model.image_tower.image_tower.encoder.layers.16.layer_norm1.weight', 'model.image_tower.image_tower.encoder.layers.10.mlp.fc1.bias', 'model.image_tower.image_tower.encoder.layers.14.self_attn.q_proj.weight', 'model.video_tower.video_tower.pre_layrnorm.weight', 'model.video_tower.video_tower.encoder.layers.2.temporal_attn.out_proj.bias', 'model.video_tower.video_tower.encoder.layers.14.self_attn.q_proj.bias', 'model.video_tower.video_tower.encoder.layers.19.mlp.fc1.weight', 'model.image_tower.image_tower.encoder.layers.10.self_attn.v_proj.weight', 'model.video_tower.video_tower.embeddings.class_embedding', 'model.video_tower.video_tower.encoder.layers.15.mlp.fc2.weight', 'model.video_tower.video_tower.encoder.layers.12.temporal_attn.out_proj.bias', 'model.video_tower.video_tower.encoder.layers.4.mlp.fc2.weight', 'model.video_tower.video_tower.encoder.layers.16.temporal_attn.out_proj.bias', 'model.image_tower.image_tower.encoder.layers.22.layer_norm2.weight', 'model.video_tower.video_tower.encoder.layers.10.mlp.fc1.bias', 'model.video_tower.video_tower.encoder.layers.1.self_attn.v_proj.bias', 'model.video_tower.video_tower.encoder.layers.16.layer_norm2.weight', 'model.video_tower.video_tower.encoder.layers.23.temporal_attn.out_proj.bias', 'model.image_tower.image_tower.encoder.layers.0.self_attn.out_proj.bias', 'model.image_tower.image_tower.encoder.layers.8.self_attn.k_proj.weight', 'model.video_tower.video_tower.encoder.layers.21.layer_norm2.bias', 'model.video_tower.video_tower.encoder.layers.20.temporal_attn.q_proj.weight', 'model.video_tower.video_tower.encoder.layers.2.layer_norm2.bias', 'model.video_tower.video_tower.encoder.layers.3.layer_norm2.weight', 'model.image_tower.image_tower.encoder.layers.13.mlp.fc1.bias', 'model.video_tower.video_tower.encoder.layers.21.temporal_attn.k_proj.weight', 'model.image_tower.image_tower.encoder.layers.22.self_attn.k_proj.weight', 'model.video_tower.video_tower.encoder.layers.22.temporal_embedding', 'model.image_tower.image_tower.encoder.layers.14.mlp.fc1.bias', 'model.video_tower.video_tower.encoder.layers.8.temporal_attn.v_proj.weight', 'model.image_tower.image_tower.encoder.layers.5.mlp.fc1.weight', 'model.video_tower.video_tower.encoder.layers.11.layer_norm2.weight', 'model.video_tower.video_tower.encoder.layers.15.temporal_attn.q_proj.weight', 'model.image_tower.image_tower.encoder.layers.4.layer_norm1.weight', 'model.video_tower.video_tower.encoder.layers.8.self_attn.q_proj.weight', 'model.image_tower.image_tower.encoder.layers.11.layer_norm2.weight', 'model.image_tower.image_tower.encoder.layers.13.mlp.fc2.bias', 'model.video_tower.video_tower.encoder.layers.23.layer_norm2.weight', 'model.video_tower.video_tower.encoder.layers.3.layer_norm1.weight', 'model.video_tower.video_tower.encoder.layers.1.temporal_attn.q_proj.weight', 'model.video_tower.video_tower.encoder.layers.21.self_attn.k_proj.bias', 'model.image_tower.image_tower.encoder.layers.14.mlp.fc1.weight', 'model.image_tower.image_tower.encoder.layers.2.self_attn.v_proj.bias', 'model.video_tower.video_tower.encoder.layers.4.self_attn.q_proj.bias', 'model.image_tower.image_tower.encoder.layers.4.self_attn.out_proj.bias', 'model.video_tower.video_tower.encoder.layers.3.self_attn.out_proj.bias', 'model.image_tower.image_tower.encoder.layers.2.self_attn.q_proj.weight', 'model.video_tower.video_tower.encoder.layers.16.temporal_layer_norm1.bias', 'model.video_tower.video_tower.encoder.layers.16.mlp.fc2.bias', 'model.image_tower.image_tower.encoder.layers.15.self_attn.out_proj.bias', 'model.video_tower.video_tower.encoder.layers.3.self_attn.q_proj.weight', 'model.video_tower.video_tower.encoder.layers.17.self_attn.k_proj.weight', 'model.video_tower.video_tower.encoder.layers.16.self_attn.out_proj.bias', 'model.video_tower.video_tower.encoder.layers.10.layer_norm2.weight', 'model.image_tower.image_tower.encoder.layers.21.mlp.fc2.bias', 'model.image_tower.image_tower.encoder.layers.3.mlp.fc2.weight', 'model.video_tower.video_tower.encoder.layers.2.layer_norm2.weight', 'model.image_tower.image_tower.encoder.layers.20.self_attn.q_proj.bias', 'model.image_tower.image_tower.encoder.layers.1.self_attn.q_proj.bias', 'model.image_tower.image_tower.encoder.layers.21.mlp.fc2.weight', 'model.image_tower.image_tower.encoder.layers.13.layer_norm1.weight', 'model.image_tower.image_tower.encoder.layers.17.self_attn.k_proj.bias', 'model.image_tower.image_tower.encoder.layers.0.mlp.fc1.bias', 'model.image_tower.image_tower.encoder.layers.3.layer_norm1.bias', 'model.video_tower.video_tower.encoder.layers.0.self_attn.v_proj.bias', 'model.image_tower.image_tower.encoder.layers.5.layer_norm2.bias', 'model.image_tower.image_tower.encoder.layers.20.mlp.fc2.bias', 'model.video_tower.video_tower.encoder.layers.0.temporal_attn.v_proj.bias', 'model.image_tower.image_tower.encoder.layers.10.mlp.fc1.weight', 'model.video_tower.video_tower.encoder.layers.15.self_attn.q_proj.weight', 'model.image_tower.image_tower.encoder.layers.4.self_attn.v_proj.bias', 'model.video_tower.video_tower.encoder.layers.11.temporal_attn.v_proj.weight', 'model.video_tower.video_tower.encoder.layers.13.temporal_attn.q_proj.weight', 'model.video_tower.video_tower.encoder.layers.15.layer_norm1.bias', 'model.image_tower.image_tower.encoder.layers.21.layer_norm2.weight', 'model.video_tower.video_tower.encoder.layers.11.mlp.fc1.bias', 'model.image_tower.image_tower.encoder.layers.9.self_attn.v_proj.bias', 'model.video_tower.video_tower.encoder.layers.18.temporal_attn.v_proj.bias', 'model.video_tower.video_tower.encoder.layers.5.temporal_layer_norm1.weight', 'model.image_tower.image_tower.encoder.layers.12.self_attn.out_proj.bias', 'model.image_tower.image_tower.encoder.layers.14.self_attn.v_proj.weight', 'model.image_tower.image_tower.encoder.layers.11.mlp.fc2.weight', 'model.video_tower.video_tower.encoder.layers.8.layer_norm2.weight', 'model.video_tower.video_tower.encoder.layers.3.temporal_attn.out_proj.bias', 'model.image_tower.image_tower.encoder.layers.9.layer_norm2.weight', 'model.image_tower.image_tower.encoder.layers.12.mlp.fc2.weight', 'model.video_tower.video_tower.encoder.layers.0.mlp.fc2.weight', 'model.image_tower.image_tower.encoder.layers.20.self_attn.out_proj.bias', 'model.video_tower.video_tower.encoder.layers.0.self_attn.k_proj.weight', 'model.video_tower.video_tower.encoder.layers.20.temporal_layer_norm1.bias', 'model.video_tower.video_tower.encoder.layers.13.temporal_attn.out_proj.weight', 'model.video_tower.video_tower.encoder.layers.13.temporal_attn.k_proj.weight', 'model.video_tower.video_tower.encoder.layers.18.temporal_layer_norm1.bias', 'model.video_tower.video_tower.encoder.layers.14.layer_norm2.bias', 'model.image_tower.image_tower.encoder.layers.0.layer_norm1.bias', 'model.video_tower.video_tower.encoder.layers.21.layer_norm1.weight', 'model.video_tower.video_tower.encoder.layers.1.self_attn.out_proj.weight', 'model.video_tower.video_tower.encoder.layers.22.mlp.fc1.bias', 'model.image_tower.image_tower.encoder.layers.7.self_attn.out_proj.bias', 'model.image_tower.image_tower.encoder.layers.5.self_attn.k_proj.weight', 'model.video_tower.video_tower.encoder.layers.12.mlp.fc2.weight', 'model.image_tower.image_tower.encoder.layers.20.self_attn.k_proj.bias', 'model.video_tower.video_tower.encoder.layers.5.mlp.fc2.weight', 'model.image_tower.image_tower.encoder.layers.17.mlp.fc1.weight', 'model.video_tower.video_tower.encoder.layers.9.layer_norm1.weight', 'model.image_tower.image_tower.encoder.layers.2.mlp.fc1.bias', 'model.video_tower.video_tower.encoder.layers.0.self_attn.q_proj.bias', 'model.video_tower.video_tower.encoder.layers.7.temporal_layer_norm1.weight', 'model.video_tower.video_tower.encoder.layers.3.temporal_attn.k_proj.bias', 'model.video_tower.video_tower.encoder.layers.7.temporal_attn.out_proj.bias', 'model.video_tower.video_tower.encoder.layers.4.mlp.fc1.bias', 'model.video_tower.video_tower.encoder.layers.3.temporal_attn.q_proj.bias', 'model.image_tower.image_tower.encoder.layers.11.mlp.fc2.bias', 'model.video_tower.video_tower.encoder.layers.5.temporal_attn.q_proj.bias', 'model.image_tower.image_tower.encoder.layers.3.self_attn.q_proj.bias', 'model.video_tower.video_tower.encoder.layers.11.self_attn.q_proj.bias', 'model.image_tower.image_tower.encoder.layers.16.layer_norm2.bias', 'model.image_tower.image_tower.encoder.layers.16.layer_norm1.bias', 'model.video_tower.video_tower.encoder.layers.2.mlp.fc2.weight', 'model.video_tower.video_tower.encoder.layers.6.self_attn.k_proj.bias', 'model.image_tower.image_tower.encoder.layers.18.self_attn.out_proj.bias', 'model.image_tower.image_tower.encoder.layers.19.self_attn.q_proj.bias', 'model.image_tower.image_tower.encoder.layers.10.self_attn.out_proj.bias', 'model.video_tower.video_tower.encoder.layers.19.temporal_layer_norm1.bias', 'model.image_tower.image_tower.encoder.layers.5.self_attn.out_proj.bias', 'model.video_tower.video_tower.encoder.layers.20.temporal_attn.out_proj.bias', 'model.video_tower.video_tower.encoder.layers.8.self_attn.k_proj.bias', 'model.video_tower.video_tower.encoder.layers.18.temporal_attn.v_proj.weight', 'model.video_tower.video_tower.encoder.layers.1.temporal_attn.v_proj.bias', 'model.video_tower.video_tower.encoder.layers.20.layer_norm2.weight', 'model.video_tower.video_tower.encoder.layers.11.temporal_attn.v_proj.bias', 'model.video_tower.video_tower.encoder.layers.1.self_attn.q_proj.weight', 'model.image_tower.image_tower.encoder.layers.23.self_attn.q_proj.weight', 'model.video_tower.video_tower.encoder.layers.19.layer_norm2.weight', 'model.video_tower.video_tower.encoder.layers.4.temporal_layer_norm1.bias', 'model.video_tower.video_tower.encoder.layers.12.mlp.fc1.weight', 'model.image_tower.image_tower.encoder.layers.20.mlp.fc1.weight', 'model.video_tower.video_tower.encoder.layers.1.layer_norm2.bias', 'model.video_tower.video_tower.encoder.layers.23.temporal_attn.v_proj.weight', 'model.video_tower.video_tower.encoder.layers.7.self_attn.q_proj.bias', 'model.image_tower.image_tower.encoder.layers.2.mlp.fc2.weight', 'model.video_tower.video_tower.encoder.layers.17.self_attn.q_proj.bias', 'model.video_tower.video_tower.encoder.layers.7.mlp.fc1.weight', 'model.image_tower.image_tower.encoder.layers.6.self_attn.v_proj.bias', 'model.video_tower.video_tower.encoder.layers.12.layer_norm1.bias', 'model.video_tower.video_tower.encoder.layers.4.layer_norm1.weight', 'model.image_tower.image_tower.encoder.layers.1.mlp.fc2.bias', 'model.video_tower.video_tower.encoder.layers.13.temporal_layer_norm1.weight', 'model.video_tower.video_tower.encoder.layers.19.mlp.fc2.weight', 'model.video_tower.video_tower.encoder.layers.3.temporal_attn.q_proj.weight', 'model.image_tower.image_tower.encoder.layers.20.self_attn.v_proj.bias', 'model.video_tower.video_tower.encoder.layers.15.self_attn.k_proj.weight', 'model.image_tower.image_tower.encoder.layers.4.self_attn.k_proj.bias', 'model.image_tower.image_tower.encoder.layers.8.self_attn.v_proj.bias', 'model.image_tower.image_tower.encoder.layers.3.self_attn.k_proj.weight', 'model.image_tower.image_tower.encoder.layers.9.mlp.fc1.weight', 'model.video_tower.video_tower.encoder.layers.9.self_attn.out_proj.weight', 'model.image_tower.image_tower.encoder.layers.5.mlp.fc1.bias', 'model.video_tower.video_tower.encoder.layers.6.layer_norm2.weight', 'model.video_tower.video_tower.encoder.layers.8.self_attn.q_proj.bias', 'model.image_tower.image_tower.encoder.layers.2.layer_norm1.weight', 'model.video_tower.video_tower.encoder.layers.7.self_attn.out_proj.weight', 'model.image_tower.image_tower.encoder.layers.17.layer_norm1.bias', 'model.video_tower.video_tower.encoder.layers.0.layer_norm2.weight', 'model.image_tower.image_tower.encoder.layers.22.self_attn.k_proj.bias', 'model.video_tower.video_tower.encoder.layers.12.temporal_layer_norm1.bias', 'model.video_tower.video_tower.encoder.layers.23.temporal_attn.v_proj.bias', 'model.video_tower.video_tower.encoder.layers.21.temporal_attn.v_proj.bias', 'model.image_tower.image_tower.encoder.layers.14.mlp.fc2.bias', 'model.image_tower.image_tower.encoder.layers.0.self_attn.k_proj.bias', 'model.image_tower.image_tower.encoder.layers.9.self_attn.k_proj.weight', 'model.video_tower.video_tower.encoder.layers.17.self_attn.v_proj.weight', 'model.video_tower.video_tower.encoder.layers.9.self_attn.out_proj.bias', 'model.video_tower.video_tower.encoder.layers.2.mlp.fc1.bias', 'model.image_tower.image_tower.encoder.layers.10.mlp.fc2.bias', 'model.video_tower.video_tower.encoder.layers.4.temporal_attn.k_proj.weight', 'model.video_tower.video_tower.encoder.layers.1.mlp.fc1.bias', 'model.image_tower.image_tower.encoder.layers.17.self_attn.k_proj.weight', 'model.video_tower.video_tower.encoder.layers.16.temporal_attn.v_proj.weight', 'model.video_tower.video_tower.encoder.layers.23.self_attn.q_proj.weight', 'model.image_tower.image_tower.encoder.layers.14.self_attn.out_proj.bias', 'model.image_tower.image_tower.encoder.layers.15.mlp.fc2.bias', 'model.image_tower.image_tower.encoder.layers.15.self_attn.q_proj.bias', 'model.video_tower.video_tower.encoder.layers.1.self_attn.q_proj.bias', 'model.image_tower.image_tower.encoder.layers.14.self_attn.q_proj.bias', 'model.video_tower.video_tower.encoder.layers.3.self_attn.k_proj.bias', 'model.image_tower.image_tower.encoder.layers.20.layer_norm2.bias', 'model.video_tower.video_tower.encoder.layers.14.self_attn.k_proj.bias', 'model.image_tower.image_tower.encoder.layers.4.layer_norm2.weight', 'model.video_tower.video_tower.encoder.layers.16.layer_norm1.bias', 'model.video_tower.video_tower.encoder.layers.13.temporal_attn.out_proj.bias', 'model.image_tower.image_tower.encoder.layers.7.layer_norm2.bias', 'model.image_tower.image_tower.encoder.layers.7.self_attn.out_proj.weight', 'model.image_tower.image_tower.encoder.layers.3.mlp.fc1.weight', 'model.video_tower.video_tower.encoder.layers.10.layer_norm1.bias', 'model.video_tower.video_tower.encoder.layers.6.temporal_attn.out_proj.bias', 'model.image_tower.image_tower.encoder.layers.22.self_attn.v_proj.weight', 'model.video_tower.video_tower.encoder.layers.15.temporal_embedding', 'model.image_tower.image_tower.post_layernorm.bias', 'model.video_tower.video_tower.encoder.layers.1.mlp.fc2.weight', 'model.video_tower.video_tower.encoder.layers.5.layer_norm2.weight', 'model.video_tower.video_tower.encoder.layers.14.temporal_layer_norm1.weight', 'model.image_tower.image_tower.encoder.layers.14.layer_norm1.bias', 'model.video_tower.video_tower.encoder.layers.11.temporal_attn.q_proj.weight', 'model.video_tower.video_tower.encoder.layers.1.temporal_attn.q_proj.bias', 'model.image_tower.image_tower.encoder.layers.11.self_attn.out_proj.weight', 'model.image_tower.image_tower.encoder.layers.8.self_attn.q_proj.weight', 'model.image_tower.image_tower.encoder.layers.11.mlp.fc1.weight', 'model.image_tower.image_tower.encoder.layers.18.self_attn.v_proj.weight', 'model.video_tower.video_tower.encoder.layers.1.temporal_attn.v_proj.weight', 'model.image_tower.image_tower.encoder.layers.10.self_attn.k_proj.weight', 'model.image_tower.image_tower.encoder.layers.7.layer_norm1.bias', 'model.video_tower.video_tower.encoder.layers.4.self_attn.k_proj.weight', 'model.video_tower.video_tower.encoder.layers.12.layer_norm1.weight', 'model.image_tower.image_tower.encoder.layers.16.self_attn.out_proj.weight', 'model.image_tower.image_tower.encoder.layers.23.layer_norm1.bias', 'model.video_tower.video_tower.encoder.layers.2.temporal_attn.k_proj.weight', 'model.video_tower.video_tower.encoder.layers.1.temporal_attn.k_proj.weight', 'model.video_tower.video_tower.encoder.layers.19.temporal_attn.v_proj.weight', 'model.video_tower.video_tower.encoder.layers.0.temporal_attn.out_proj.weight', 'model.video_tower.video_tower.encoder.layers.4.self_attn.out_proj.weight', 'model.video_tower.video_tower.encoder.layers.22.temporal_attn.q_proj.bias', 'model.video_tower.video_tower.encoder.layers.18.temporal_attn.q_proj.bias', 'model.video_tower.video_tower.encoder.layers.22.mlp.fc1.weight', 'model.video_tower.video_tower.encoder.layers.6.mlp.fc2.weight', 'model.image_tower.image_tower.encoder.layers.18.self_attn.k_proj.bias', 'model.video_tower.video_tower.encoder.layers.13.layer_norm1.bias', 'model.video_tower.video_tower.encoder.layers.7.temporal_attn.q_proj.bias', 'model.image_tower.image_tower.encoder.layers.22.self_attn.out_proj.weight', 'model.image_tower.image_tower.encoder.layers.18.self_attn.v_proj.bias', 'model.video_tower.video_tower.encoder.layers.11.temporal_attn.k_proj.weight', 'model.video_tower.video_tower.encoder.layers.5.temporal_attn.k_proj.bias', 'model.image_tower.image_tower.encoder.layers.21.self_attn.out_proj.weight', 'model.image_tower.image_tower.encoder.layers.21.self_attn.out_proj.bias', 'model.image_tower.image_tower.encoder.layers.19.mlp.fc1.bias', 'model.video_tower.video_tower.encoder.layers.4.self_attn.v_proj.weight', 'model.video_tower.video_tower.encoder.layers.14.mlp.fc1.bias', 'model.image_tower.image_tower.encoder.layers.16.layer_norm2.weight', 'model.image_tower.image_tower.encoder.layers.1.self_attn.out_proj.weight', 'model.video_tower.video_tower.encoder.layers.22.mlp.fc2.bias', 'model.video_tower.video_tower.encoder.layers.19.self_attn.k_proj.weight', 'model.video_tower.video_tower.encoder.layers.4.temporal_attn.v_proj.bias', 'model.image_tower.image_tower.encoder.layers.7.self_attn.q_proj.weight', 'model.video_tower.video_tower.encoder.layers.17.self_attn.out_proj.bias', 'model.video_tower.video_tower.encoder.layers.15.self_attn.out_proj.weight', 'model.image_tower.image_tower.encoder.layers.2.layer_norm2.bias', 'model.image_tower.image_tower.encoder.layers.12.mlp.fc1.bias', 'model.video_tower.video_tower.encoder.layers.8.temporal_layer_norm1.weight', 'model.video_tower.video_tower.encoder.layers.18.layer_norm2.weight', 'model.video_tower.video_tower.encoder.layers.15.temporal_layer_norm1.bias', 'model.video_tower.video_tower.encoder.layers.1.temporal_layer_norm1.weight', 'model.video_tower.video_tower.encoder.layers.22.self_attn.q_proj.weight', 'model.image_tower.image_tower.encoder.layers.12.layer_norm1.bias', 'model.image_tower.image_tower.encoder.layers.7.layer_norm2.weight', 'model.video_tower.video_tower.encoder.layers.22.self_attn.out_proj.bias', 'model.video_tower.video_tower.encoder.layers.18.layer_norm2.bias', 'model.image_tower.image_tower.encoder.layers.16.self_attn.k_proj.bias', 'model.video_tower.video_tower.encoder.layers.8.layer_norm2.bias', 'model.video_tower.video_tower.embeddings.patch_embedding.weight', 'model.video_tower.video_tower.encoder.layers.5.self_attn.k_proj.weight', 'model.video_tower.video_tower.encoder.layers.19.self_attn.q_proj.bias', 'model.image_tower.image_tower.encoder.layers.1.layer_norm2.weight', 'model.video_tower.video_tower.encoder.layers.4.temporal_embedding', 'model.video_tower.video_tower.encoder.layers.22.temporal_attn.v_proj.bias', 'model.image_tower.image_tower.encoder.layers.10.layer_norm1.weight', 'model.image_tower.image_tower.encoder.layers.23.self_attn.k_proj.weight', 'model.video_tower.video_tower.encoder.layers.0.self_attn.k_proj.bias', 'model.image_tower.image_tower.encoder.layers.19.layer_norm2.weight', 'model.image_tower.image_tower.encoder.layers.18.mlp.fc1.bias', 'model.video_tower.video_tower.encoder.layers.22.temporal_attn.out_proj.weight', 'model.image_tower.image_tower.encoder.layers.23.self_attn.out_proj.bias', 'model.image_tower.image_tower.encoder.layers.18.self_attn.q_proj.weight', 'model.video_tower.video_tower.encoder.layers.19.self_attn.q_proj.weight', 'model.video_tower.video_tower.encoder.layers.15.temporal_attn.k_proj.bias', 'model.video_tower.video_tower.encoder.layers.4.temporal_attn.out_proj.weight', 'model.video_tower.video_tower.encoder.layers.10.temporal_attn.q_proj.weight', 'model.video_tower.video_tower.encoder.layers.21.layer_norm2.weight', 'model.video_tower.video_tower.encoder.layers.21.self_attn.q_proj.weight', 'model.image_tower.image_tower.encoder.layers.4.self_attn.k_proj.weight', 'model.video_tower.video_tower.encoder.layers.15.mlp.fc2.bias', 'model.video_tower.video_tower.encoder.layers.18.self_attn.k_proj.bias', 'model.video_tower.video_tower.encoder.layers.14.temporal_attn.v_proj.bias', 'model.video_tower.video_tower.encoder.layers.3.temporal_attn.v_proj.weight', 'model.video_tower.video_tower.encoder.layers.2.temporal_attn.out_proj.weight', 'model.image_tower.image_tower.encoder.layers.4.self_attn.q_proj.weight', 'model.video_tower.video_tower.encoder.layers.10.temporal_layer_norm1.weight', 'model.image_tower.image_tower.encoder.layers.23.self_attn.k_proj.bias', 'model.video_tower.video_tower.encoder.layers.23.self_attn.out_proj.weight', 'model.image_tower.image_tower.encoder.layers.8.mlp.fc1.bias', 'model.video_tower.video_tower.encoder.layers.3.temporal_layer_norm1.weight', 'model.video_tower.video_tower.post_layernorm.bias', 'model.video_tower.video_tower.encoder.layers.2.self_attn.v_proj.weight', 'model.image_tower.image_tower.encoder.layers.9.self_attn.out_proj.weight', 'model.video_tower.video_tower.encoder.layers.19.self_attn.out_proj.bias', 'model.image_tower.image_tower.encoder.layers.2.self_attn.k_proj.weight', 'model.video_tower.video_tower.encoder.layers.11.temporal_attn.out_proj.bias', 'model.video_tower.video_tower.encoder.layers.0.temporal_layer_norm1.weight', 'model.video_tower.video_tower.encoder.layers.5.mlp.fc1.weight', 'model.video_tower.video_tower.encoder.layers.14.self_attn.q_proj.weight', 'model.video_tower.video_tower.encoder.layers.7.layer_norm2.bias', 'model.video_tower.video_tower.encoder.layers.16.mlp.fc1.bias', 'model.image_tower.image_tower.encoder.layers.5.mlp.fc2.weight', 'model.video_tower.video_tower.encoder.layers.20.layer_norm2.bias', 'model.video_tower.video_tower.encoder.layers.14.temporal_attn.q_proj.bias', 'model.video_tower.video_tower.encoder.layers.18.self_attn.q_proj.weight', 'model.video_tower.video_tower.encoder.layers.15.temporal_attn.q_proj.bias', 'model.image_tower.image_tower.encoder.layers.5.self_attn.v_proj.bias', 'model.video_tower.video_tower.encoder.layers.23.self_attn.k_proj.bias', 'model.image_tower.image_tower.encoder.layers.1.self_attn.k_proj.bias', 'model.image_tower.image_tower.pre_layrnorm.bias', 'model.image_tower.image_tower.encoder.layers.0.self_attn.k_proj.weight', 'model.video_tower.video_tower.encoder.layers.19.temporal_attn.k_proj.weight', 'model.image_tower.image_tower.encoder.layers.21.layer_norm1.bias', 'model.video_tower.video_tower.encoder.layers.0.temporal_attn.out_proj.bias', 'model.video_tower.video_tower.encoder.layers.5.temporal_attn.v_proj.weight', 'model.video_tower.video_tower.encoder.layers.23.temporal_layer_norm1.bias', 'model.video_tower.video_tower.encoder.layers.0.temporal_layer_norm1.bias', 'model.video_tower.video_tower.encoder.layers.14.self_attn.v_proj.bias', 'model.image_tower.image_tower.encoder.layers.7.layer_norm1.weight', 'model.video_tower.video_tower.encoder.layers.19.self_attn.v_proj.weight', 'model.video_tower.video_tower.encoder.layers.20.self_attn.out_proj.weight', 'model.image_tower.image_tower.encoder.layers.17.layer_norm2.bias', 'model.image_tower.image_tower.encoder.layers.6.self_attn.k_proj.bias', 'model.video_tower.video_tower.encoder.layers.3.self_attn.q_proj.bias', 'model.video_tower.video_tower.encoder.layers.4.self_attn.out_proj.bias', 'model.video_tower.video_tower.encoder.layers.2.temporal_layer_norm1.weight', 'model.video_tower.video_tower.encoder.layers.12.self_attn.k_proj.bias', 'model.video_tower.video_tower.encoder.layers.0.self_attn.v_proj.weight', 'model.video_tower.video_tower.encoder.layers.0.mlp.fc1.bias', 'model.image_tower.image_tower.encoder.layers.23.layer_norm1.weight', 'model.image_tower.image_tower.encoder.layers.20.self_attn.q_proj.weight', 'model.video_tower.video_tower.encoder.layers.12.temporal_attn.v_proj.weight', 'model.video_tower.video_tower.encoder.layers.18.temporal_attn.k_proj.bias', 'model.video_tower.video_tower.encoder.layers.15.mlp.fc1.bias', 'model.video_tower.video_tower.encoder.layers.9.mlp.fc2.bias', 'model.video_tower.video_tower.encoder.layers.4.temporal_attn.out_proj.bias', 'model.image_tower.image_tower.encoder.layers.13.self_attn.k_proj.weight', 'model.image_tower.image_tower.encoder.layers.0.self_attn.v_proj.weight', 'model.image_tower.image_tower.encoder.layers.5.self_attn.q_proj.weight', 'model.image_tower.image_tower.encoder.layers.4.self_attn.v_proj.weight', 'model.video_tower.video_tower.encoder.layers.23.mlp.fc2.bias', 'model.image_tower.image_tower.encoder.layers.0.mlp.fc1.weight', 'model.image_tower.image_tower.encoder.layers.2.mlp.fc2.bias', 'model.video_tower.video_tower.encoder.layers.8.temporal_attn.v_proj.bias', 'model.image_tower.image_tower.encoder.layers.19.self_attn.v_proj.bias', 'model.video_tower.video_tower.encoder.layers.3.temporal_attn.out_proj.weight', 'model.video_tower.video_tower.encoder.layers.18.temporal_attn.q_proj.weight', 'model.image_tower.image_tower.encoder.layers.13.layer_norm2.weight', 'model.video_tower.video_tower.encoder.layers.5.self_attn.v_proj.weight', 'model.video_tower.video_tower.encoder.layers.8.mlp.fc1.bias', 'model.video_tower.video_tower.encoder.layers.16.mlp.fc2.weight', 'model.video_tower.video_tower.encoder.layers.7.temporal_attn.q_proj.weight', 'model.image_tower.image_tower.encoder.layers.21.layer_norm1.weight', 'model.video_tower.video_tower.encoder.layers.22.temporal_attn.k_proj.weight', 'model.video_tower.video_tower.encoder.layers.21.temporal_attn.q_proj.weight', 'model.image_tower.image_tower.encoder.layers.13.self_attn.q_proj.weight', 'model.video_tower.video_tower.encoder.layers.5.self_attn.out_proj.weight', 'model.video_tower.video_tower.encoder.layers.9.self_attn.q_proj.weight', 'model.image_tower.image_tower.encoder.layers.21.self_attn.q_proj.weight', 'model.video_tower.video_tower.encoder.layers.22.layer_norm2.bias', 'model.image_tower.image_tower.encoder.layers.5.layer_norm1.weight', 'model.video_tower.video_tower.encoder.layers.10.self_attn.q_proj.weight', 'model.video_tower.video_tower.encoder.layers.14.self_attn.k_proj.weight', 'model.video_tower.video_tower.encoder.layers.4.layer_norm2.weight', 'model.video_tower.video_tower.encoder.layers.16.temporal_attn.q_proj.bias', 'model.video_tower.video_tower.encoder.layers.19.self_attn.out_proj.weight', 'model.image_tower.image_tower.encoder.layers.3.self_attn.v_proj.weight', 'model.image_tower.image_tower.encoder.layers.11.layer_norm1.weight', 'model.image_tower.image_tower.encoder.layers.6.mlp.fc2.bias', 'model.video_tower.video_tower.encoder.layers.4.self_attn.v_proj.bias', 'model.image_tower.image_tower.encoder.layers.12.layer_norm2.bias', 'model.image_tower.image_tower.encoder.layers.16.self_attn.v_proj.weight', 'model.video_tower.video_tower.encoder.layers.10.temporal_attn.k_proj.weight', 'model.image_tower.image_tower.encoder.layers.5.self_attn.k_proj.bias', 'model.video_tower.video_tower.encoder.layers.9.layer_norm2.bias', 'model.image_tower.image_tower.encoder.layers.4.mlp.fc1.weight', 'model.video_tower.video_tower.encoder.layers.13.layer_norm2.weight', 'model.video_tower.video_tower.encoder.layers.23.self_attn.v_proj.bias', 'model.video_tower.video_tower.encoder.layers.17.self_attn.q_proj.weight', 'model.image_tower.image_tower.encoder.layers.7.self_attn.v_proj.bias', 'model.video_tower.video_tower.encoder.layers.2.temporal_attn.q_proj.weight', 'model.image_tower.image_tower.encoder.layers.16.mlp.fc1.bias', 'model.image_tower.image_tower.encoder.layers.4.layer_norm2.bias', 'model.video_tower.video_tower.encoder.layers.19.self_attn.v_proj.bias', 'model.video_tower.video_tower.encoder.layers.12.layer_norm2.weight', 'model.video_tower.video_tower.encoder.layers.12.temporal_attn.k_proj.bias', 'model.video_tower.video_tower.encoder.layers.14.layer_norm2.weight', 'model.video_tower.video_tower.encoder.layers.16.temporal_layer_norm1.weight', 'model.image_tower.image_tower.encoder.layers.3.self_attn.v_proj.bias', 'model.video_tower.video_tower.encoder.layers.17.temporal_attn.k_proj.bias', 'model.video_tower.video_tower.encoder.layers.20.temporal_attn.k_proj.weight', 'model.video_tower.video_tower.encoder.layers.2.self_attn.out_proj.bias', 'model.video_tower.video_tower.encoder.layers.7.temporal_layer_norm1.bias', 'model.image_tower.image_tower.encoder.layers.16.mlp.fc2.bias', 'model.image_tower.image_tower.encoder.layers.23.mlp.fc1.weight', 'model.video_tower.video_tower.encoder.layers.20.temporal_attn.out_proj.weight', 'model.video_tower.video_tower.encoder.layers.17.layer_norm2.weight', 'model.image_tower.image_tower.encoder.layers.8.self_attn.out_proj.bias', 'model.video_tower.video_tower.encoder.layers.18.self_attn.out_proj.weight', 'model.video_tower.video_tower.encoder.layers.2.self_attn.k_proj.bias', 'model.video_tower.video_tower.encoder.layers.20.self_attn.k_proj.weight', 'model.video_tower.video_tower.encoder.layers.5.self_attn.q_proj.weight', 'model.video_tower.video_tower.encoder.layers.7.layer_norm2.weight', 'model.video_tower.video_tower.encoder.layers.1.temporal_layer_norm1.bias', 'model.video_tower.video_tower.encoder.layers.1.layer_norm1.bias', 'model.video_tower.video_tower.encoder.layers.21.mlp.fc2.bias', 'model.video_tower.video_tower.encoder.layers.8.layer_norm1.bias', 'model.video_tower.video_tower.encoder.layers.21.mlp.fc1.bias', 'model.video_tower.video_tower.encoder.layers.1.temporal_attn.k_proj.bias', 'model.video_tower.video_tower.encoder.layers.6.temporal_layer_norm1.weight', 'model.video_tower.video_tower.encoder.layers.22.self_attn.k_proj.bias', 'model.video_tower.video_tower.encoder.layers.21.temporal_attn.q_proj.bias', 'model.video_tower.video_tower.encoder.layers.0.temporal_embedding', 'model.video_tower.video_tower.encoder.layers.6.temporal_attn.q_proj.bias', 'model.image_tower.image_tower.encoder.layers.13.mlp.fc1.weight', 'model.image_tower.image_tower.encoder.layers.9.self_attn.out_proj.bias', 'model.image_tower.image_tower.encoder.layers.23.self_attn.v_proj.weight', 'model.video_tower.video_tower.encoder.layers.1.temporal_embedding', 'model.video_tower.video_tower.encoder.layers.21.self_attn.v_proj.bias', 'model.video_tower.video_tower.encoder.layers.1.self_attn.out_proj.bias', 'model.video_tower.video_tower.encoder.layers.16.layer_norm2.bias', 'model.image_tower.image_tower.encoder.layers.4.mlp.fc1.bias', 'model.image_tower.image_tower.encoder.layers.9.mlp.fc2.weight', 'model.video_tower.video_tower.encoder.layers.19.layer_norm1.weight', 'model.image_tower.image_tower.encoder.layers.14.self_attn.v_proj.bias', 'model.image_tower.image_tower.encoder.layers.17.layer_norm2.weight', 'model.video_tower.video_tower.encoder.layers.16.self_attn.q_proj.bias', 'model.video_tower.video_tower.encoder.layers.23.mlp.fc2.weight', 'model.image_tower.image_tower.encoder.layers.11.self_attn.v_proj.weight', 'model.video_tower.video_tower.encoder.layers.13.self_attn.out_proj.weight', 'model.image_tower.image_tower.encoder.layers.1.layer_norm1.weight', 'model.video_tower.video_tower.encoder.layers.4.mlp.fc1.weight', 'model.image_tower.image_tower.encoder.layers.15.mlp.fc1.weight', 'model.video_tower.video_tower.encoder.layers.15.self_attn.out_proj.bias', 'model.image_tower.image_tower.encoder.layers.5.layer_norm2.weight', 'model.image_tower.image_tower.encoder.layers.19.self_attn.k_proj.weight', 'model.image_tower.image_tower.encoder.layers.10.self_attn.q_proj.bias', 'model.image_tower.image_tower.encoder.layers.18.layer_norm2.bias', 'model.video_tower.video_tower.encoder.layers.22.mlp.fc2.weight', 'model.image_tower.image_tower.encoder.layers.7.mlp.fc1.bias', 'model.video_tower.video_tower.encoder.layers.20.mlp.fc1.weight', 'model.video_tower.video_tower.encoder.layers.2.mlp.fc2.bias', 'model.image_tower.image_tower.encoder.layers.8.layer_norm1.bias', 'model.video_tower.video_tower.encoder.layers.9.layer_norm1.bias', 'model.video_tower.video_tower.encoder.layers.17.temporal_attn.out_proj.bias', 'model.image_tower.image_tower.encoder.layers.6.layer_norm1.bias', 'model.image_tower.image_tower.encoder.layers.15.mlp.fc1.bias', 'model.video_tower.video_tower.encoder.layers.2.self_attn.out_proj.weight', 'model.image_tower.image_tower.encoder.layers.12.layer_norm2.weight', 'model.video_tower.video_tower.encoder.layers.5.mlp.fc2.bias', 'model.video_tower.video_tower.encoder.layers.2.layer_norm1.bias', 'model.image_tower.image_tower.encoder.layers.16.self_attn.q_proj.weight', 'model.image_tower.image_tower.encoder.layers.15.self_attn.k_proj.weight', 'model.image_tower.image_tower.encoder.layers.7.mlp.fc1.weight', 'model.image_tower.image_tower.encoder.layers.22.layer_norm1.weight', 'model.image_tower.image_tower.encoder.layers.2.self_attn.q_proj.bias', 'model.video_tower.video_tower.encoder.layers.9.temporal_attn.out_proj.weight', 'model.video_tower.video_tower.encoder.layers.16.layer_norm1.weight', 'model.video_tower.video_tower.encoder.layers.18.self_attn.out_proj.bias', 'model.video_tower.video_tower.encoder.layers.6.temporal_attn.q_proj.weight', 'model.image_tower.image_tower.encoder.layers.6.self_attn.v_proj.weight', 'model.image_tower.image_tower.encoder.layers.12.self_attn.v_proj.weight', 'model.image_tower.image_tower.encoder.layers.15.layer_norm1.bias', 'model.video_tower.video_tower.encoder.layers.8.mlp.fc1.weight', 'model.video_tower.video_tower.encoder.layers.12.temporal_attn.q_proj.bias', 'model.video_tower.video_tower.encoder.layers.7.mlp.fc1.bias', 'model.video_tower.video_tower.encoder.layers.14.temporal_embedding', 'model.video_tower.video_tower.encoder.layers.21.mlp.fc2.weight', 'model.video_tower.video_tower.encoder.layers.9.self_attn.k_proj.bias', 'model.video_tower.video_tower.encoder.layers.8.mlp.fc2.bias', 'model.image_tower.image_tower.encoder.layers.19.layer_norm1.weight', 'model.video_tower.video_tower.encoder.layers.17.temporal_layer_norm1.bias', 'model.image_tower.image_tower.encoder.layers.19.self_attn.k_proj.bias', 'model.video_tower.video_tower.encoder.layers.22.temporal_attn.q_proj.weight', 'model.video_tower.video_tower.encoder.layers.15.self_attn.q_proj.bias', 'model.video_tower.video_tower.encoder.layers.19.layer_norm2.bias', 'model.video_tower.video_tower.encoder.layers.18.mlp.fc2.weight', 'model.image_tower.image_tower.encoder.layers.14.layer_norm2.bias', 'model.video_tower.video_tower.encoder.layers.14.mlp.fc2.bias', 'model.image_tower.image_tower.encoder.layers.15.self_attn.q_proj.weight', 'model.image_tower.image_tower.encoder.layers.6.mlp.fc1.bias', 'model.image_tower.image_tower.encoder.layers.23.self_attn.out_proj.weight', 'model.image_tower.image_tower.encoder.layers.19.mlp.fc2.weight', 'model.video_tower.video_tower.encoder.layers.9.mlp.fc1.weight', 'model.video_tower.video_tower.encoder.layers.3.layer_norm1.bias', 'model.video_tower.video_tower.encoder.layers.23.temporal_layer_norm1.weight', 'model.video_tower.video_tower.encoder.layers.12.temporal_attn.q_proj.weight', 'model.video_tower.video_tower.encoder.layers.17.temporal_attn.v_proj.bias', 'model.video_tower.video_tower.encoder.layers.7.self_attn.out_proj.bias', 'model.video_tower.video_tower.encoder.layers.9.temporal_attn.v_proj.weight', 'model.image_tower.image_tower.encoder.layers.9.self_attn.v_proj.weight', 'model.video_tower.video_tower.encoder.layers.14.mlp.fc1.weight', 'model.video_tower.video_tower.encoder.layers.6.mlp.fc1.bias', 'model.image_tower.image_tower.encoder.layers.20.self_attn.out_proj.weight', 'model.video_tower.video_tower.encoder.layers.4.self_attn.q_proj.weight', 'model.video_tower.video_tower.encoder.layers.5.layer_norm1.bias', 'model.video_tower.video_tower.encoder.layers.21.temporal_attn.out_proj.bias', 'model.video_tower.video_tower.encoder.layers.19.temporal_attn.out_proj.weight', 'model.video_tower.video_tower.encoder.layers.13.self_attn.q_proj.weight', 'model.video_tower.video_tower.encoder.layers.13.temporal_attn.v_proj.bias', 'model.video_tower.video_tower.encoder.layers.5.temporal_attn.k_proj.weight', 'model.image_tower.image_tower.encoder.layers.11.self_attn.v_proj.bias', 'model.video_tower.video_tower.encoder.layers.4.self_attn.k_proj.bias', 'model.video_tower.video_tower.encoder.layers.9.layer_norm2.weight', 'model.video_tower.video_tower.encoder.layers.11.temporal_layer_norm1.bias', 'model.video_tower.video_tower.encoder.layers.12.mlp.fc1.bias', 'model.video_tower.video_tower.encoder.layers.7.layer_norm1.bias', 'model.image_tower.image_tower.encoder.layers.22.self_attn.v_proj.bias', 'model.video_tower.video_tower.encoder.layers.4.mlp.fc2.bias', 'model.video_tower.video_tower.encoder.layers.21.self_attn.k_proj.weight', 'model.video_tower.video_tower.encoder.layers.9.mlp.fc2.weight', 'model.video_tower.video_tower.pre_layrnorm.bias', 'model.video_tower.video_tower.encoder.layers.13.mlp.fc1.weight', 'model.image_tower.image_tower.encoder.layers.17.self_attn.q_proj.weight', 'model.video_tower.video_tower.encoder.layers.11.temporal_layer_norm1.weight', 'model.video_tower.video_tower.encoder.layers.7.self_attn.v_proj.bias', 'model.video_tower.video_tower.encoder.layers.17.temporal_attn.q_proj.weight', 'model.image_tower.image_tower.encoder.layers.1.mlp.fc2.weight', 'model.video_tower.video_tower.encoder.layers.21.layer_norm1.bias', 'model.video_tower.video_tower.encoder.layers.11.self_attn.v_proj.weight', 'model.video_tower.video_tower.encoder.layers.10.self_attn.out_proj.weight', 'model.image_tower.image_tower.embeddings.patch_embedding.weight', 'model.video_tower.video_tower.encoder.layers.16.self_attn.k_proj.bias', 'model.image_tower.image_tower.encoder.layers.5.self_attn.out_proj.weight', 'model.video_tower.video_tower.encoder.layers.14.temporal_layer_norm1.bias', 'model.video_tower.video_tower.encoder.layers.0.temporal_attn.k_proj.bias', 'model.image_tower.image_tower.encoder.layers.15.self_attn.v_proj.bias', 'model.image_tower.image_tower.encoder.layers.6.layer_norm2.bias', 'model.video_tower.video_tower.encoder.layers.16.temporal_attn.v_proj.bias', 'model.image_tower.image_tower.encoder.layers.19.mlp.fc1.weight', 'model.video_tower.video_tower.encoder.layers.8.temporal_attn.out_proj.bias', 'model.image_tower.image_tower.encoder.layers.19.mlp.fc2.bias', 'model.image_tower.image_tower.encoder.layers.12.layer_norm1.weight', 'model.video_tower.video_tower.encoder.layers.8.temporal_attn.q_proj.bias', 'model.video_tower.video_tower.encoder.layers.12.temporal_attn.out_proj.weight', 'model.video_tower.video_tower.encoder.layers.15.mlp.fc1.weight', 'model.video_tower.video_tower.encoder.layers.20.self_attn.v_proj.weight', 'model.image_tower.image_tower.encoder.layers.23.self_attn.q_proj.bias', 'model.image_tower.image_tower.encoder.layers.21.mlp.fc1.weight', 'model.image_tower.image_tower.encoder.layers.0.layer_norm2.bias', 'model.image_tower.image_tower.encoder.layers.16.mlp.fc2.weight', 'model.image_tower.image_tower.encoder.layers.17.self_attn.q_proj.bias', 'model.image_tower.image_tower.encoder.layers.19.self_attn.q_proj.weight', 'model.image_tower.image_tower.encoder.layers.18.mlp.fc1.weight', 'model.video_tower.video_tower.encoder.layers.12.self_attn.q_proj.weight', 'model.image_tower.image_tower.encoder.layers.1.layer_norm1.bias', 'model.video_tower.video_tower.encoder.layers.14.layer_norm1.bias', 'model.video_tower.video_tower.encoder.layers.23.temporal_attn.k_proj.bias', 'model.image_tower.image_tower.encoder.layers.10.self_attn.out_proj.weight', 'model.video_tower.video_tower.encoder.layers.14.self_attn.v_proj.weight', 'model.video_tower.video_tower.encoder.layers.15.self_attn.v_proj.bias', 'model.image_tower.image_tower.encoder.layers.17.self_attn.out_proj.weight', 'model.image_tower.image_tower.embeddings.position_embedding.weight', 'model.video_tower.video_tower.encoder.layers.21.mlp.fc1.weight', 'model.image_tower.image_tower.encoder.layers.10.self_attn.q_proj.weight', 'model.image_tower.image_tower.encoder.layers.17.mlp.fc2.weight', 'model.image_tower.image_tower.encoder.layers.9.self_attn.q_proj.bias', 'model.image_tower.image_tower.encoder.layers.6.self_attn.out_proj.weight', 'model.image_tower.image_tower.encoder.layers.3.self_attn.q_proj.weight', 'model.image_tower.image_tower.encoder.layers.11.self_attn.q_proj.weight', 'model.image_tower.image_tower.encoder.layers.14.layer_norm2.weight', 'model.image_tower.image_tower.encoder.layers.7.self_attn.v_proj.weight', 'model.video_tower.video_tower.encoder.layers.20.self_attn.v_proj.bias', 'model.image_tower.image_tower.encoder.layers.8.layer_norm2.weight', 'model.video_tower.video_tower.encoder.layers.17.layer_norm1.bias', 'model.image_tower.image_tower.encoder.layers.0.mlp.fc2.weight', 'model.image_tower.image_tower.encoder.layers.20.mlp.fc1.bias', 'model.video_tower.video_tower.encoder.layers.15.self_attn.k_proj.bias', 'model.image_tower.image_tower.encoder.layers.9.layer_norm1.bias', 'model.video_tower.video_tower.encoder.layers.0.self_attn.out_proj.weight', 'model.video_tower.video_tower.encoder.layers.17.mlp.fc2.weight', 'model.video_tower.video_tower.encoder.layers.21.self_attn.q_proj.bias', 'model.video_tower.video_tower.encoder.layers.14.temporal_attn.k_proj.bias', 'model.video_tower.video_tower.encoder.layers.15.temporal_layer_norm1.weight', 'model.video_tower.video_tower.encoder.layers.21.self_attn.out_proj.weight', 'model.image_tower.image_tower.encoder.layers.15.self_attn.k_proj.bias', 'model.video_tower.video_tower.encoder.layers.20.temporal_layer_norm1.weight', 'model.image_tower.image_tower.encoder.layers.20.layer_norm1.weight', 'model.image_tower.image_tower.encoder.layers.20.mlp.fc2.weight', 'model.image_tower.image_tower.encoder.layers.9.self_attn.k_proj.bias', 'model.video_tower.video_tower.encoder.layers.19.temporal_embedding', 'model.video_tower.video_tower.encoder.layers.23.layer_norm1.weight', 'model.video_tower.video_tower.encoder.layers.16.mlp.fc1.weight', 'model.video_tower.video_tower.encoder.layers.3.self_attn.out_proj.weight', 'model.image_tower.image_tower.encoder.layers.13.self_attn.v_proj.weight', 'model.video_tower.video_tower.encoder.layers.3.temporal_attn.v_proj.bias', 'model.video_tower.video_tower.encoder.layers.8.layer_norm1.weight', 'model.video_tower.video_tower.encoder.layers.20.mlp.fc2.weight', 'model.video_tower.video_tower.encoder.layers.18.self_attn.k_proj.weight', 'model.video_tower.video_tower.encoder.layers.12.temporal_attn.k_proj.weight', 'model.image_tower.image_tower.encoder.layers.12.self_attn.out_proj.weight', 'model.image_tower.image_tower.encoder.layers.15.layer_norm2.weight', 'model.video_tower.video_tower.encoder.layers.19.temporal_attn.v_proj.bias', 'model.video_tower.video_tower.encoder.layers.17.temporal_attn.k_proj.weight', 'model.image_tower.image_tower.encoder.layers.17.layer_norm1.weight', 'model.video_tower.video_tower.encoder.layers.23.self_attn.q_proj.bias', 'model.image_tower.image_tower.encoder.layers.3.layer_norm2.weight', 'model.video_tower.video_tower.encoder.layers.23.layer_norm1.bias', 'model.image_tower.image_tower.encoder.layers.16.self_attn.v_proj.bias', 'model.video_tower.video_tower.encoder.layers.8.temporal_attn.q_proj.weight', 'model.video_tower.video_tower.encoder.layers.18.mlp.fc1.bias', 'model.video_tower.video_tower.encoder.layers.22.temporal_layer_norm1.bias', 'model.video_tower.video_tower.encoder.layers.15.temporal_attn.v_proj.weight', 'model.video_tower.video_tower.encoder.layers.20.self_attn.q_proj.weight', 'model.image_tower.image_tower.encoder.layers.10.self_attn.k_proj.bias', 'model.image_tower.image_tower.encoder.layers.19.self_attn.out_proj.bias', 'model.video_tower.video_tower.encoder.layers.22.temporal_attn.v_proj.weight', 'model.video_tower.video_tower.encoder.layers.13.temporal_attn.q_proj.bias', 'model.video_tower.video_tower.encoder.layers.20.self_attn.q_proj.bias', 'model.video_tower.video_tower.encoder.layers.18.temporal_embedding', 'model.video_tower.video_tower.encoder.layers.6.self_attn.k_proj.weight', 'model.image_tower.image_tower.encoder.layers.14.mlp.fc2.weight', 'model.video_tower.video_tower.encoder.layers.4.layer_norm2.bias', 'model.video_tower.video_tower.encoder.layers.6.mlp.fc1.weight', 'model.video_tower.video_tower.encoder.layers.12.mlp.fc2.bias', 'model.image_tower.image_tower.encoder.layers.5.self_attn.q_proj.bias', 'model.image_tower.image_tower.encoder.layers.10.self_attn.v_proj.bias', 'model.image_tower.image_tower.encoder.layers.0.mlp.fc2.bias', 'model.video_tower.video_tower.encoder.layers.1.mlp.fc2.bias', 'model.video_tower.video_tower.encoder.layers.4.temporal_attn.v_proj.weight', 'model.video_tower.video_tower.encoder.layers.15.temporal_attn.v_proj.bias', 'model.video_tower.video_tower.encoder.layers.15.temporal_attn.out_proj.weight', 'model.video_tower.video_tower.encoder.layers.6.self_attn.q_proj.weight', 'model.video_tower.video_tower.encoder.layers.7.temporal_attn.out_proj.weight', 'model.image_tower.image_tower.encoder.layers.3.mlp.fc1.bias', 'model.image_tower.image_tower.encoder.layers.8.mlp.fc2.weight', 'model.video_tower.video_tower.encoder.layers.17.layer_norm2.bias', 'model.video_tower.video_tower.encoder.layers.17.self_attn.k_proj.bias', 'model.video_tower.video_tower.encoder.layers.9.temporal_attn.k_proj.bias', 'model.image_tower.image_tower.encoder.layers.23.self_attn.v_proj.bias', 'model.video_tower.video_tower.encoder.layers.17.mlp.fc1.weight', 'model.video_tower.video_tower.encoder.layers.13.self_attn.q_proj.bias', 'model.image_tower.image_tower.encoder.layers.4.self_attn.q_proj.bias', 'model.image_tower.image_tower.encoder.layers.12.self_attn.q_proj.weight', 'model.image_tower.image_tower.encoder.layers.3.layer_norm2.bias', 'model.video_tower.video_tower.encoder.layers.10.self_attn.v_proj.weight', 'model.image_tower.image_tower.encoder.layers.8.layer_norm2.bias', 'model.image_tower.image_tower.encoder.layers.12.self_attn.k_proj.weight', 'model.video_tower.video_tower.encoder.layers.5.mlp.fc1.bias', 'model.image_tower.image_tower.encoder.layers.1.self_attn.k_proj.weight', 'model.video_tower.video_tower.encoder.layers.20.temporal_attn.v_proj.bias', 'model.image_tower.image_tower.encoder.layers.4.layer_norm1.bias', 'model.image_tower.image_tower.encoder.layers.16.self_attn.k_proj.weight', 'model.image_tower.image_tower.encoder.layers.9.layer_norm2.bias', 'model.video_tower.video_tower.encoder.layers.7.self_attn.q_proj.weight', 'model.video_tower.video_tower.encoder.layers.4.temporal_attn.q_proj.weight', 'model.video_tower.video_tower.encoder.layers.7.self_attn.k_proj.weight', 'model.video_tower.video_tower.encoder.layers.20.mlp.fc1.bias', 'model.video_tower.video_tower.encoder.layers.2.self_attn.k_proj.weight', 'model.image_tower.image_tower.encoder.layers.0.self_attn.out_proj.weight', 'model.video_tower.video_tower.encoder.layers.15.temporal_attn.k_proj.weight', 'model.video_tower.video_tower.encoder.layers.17.temporal_attn.v_proj.weight', 'model.video_tower.video_tower.encoder.layers.4.temporal_attn.k_proj.bias', 'model.image_tower.image_tower.encoder.layers.14.self_attn.out_proj.weight', 'model.image_tower.image_tower.encoder.layers.18.layer_norm1.weight', 'model.video_tower.video_tower.encoder.layers.6.temporal_attn.k_proj.bias', 'model.video_tower.video_tower.encoder.layers.16.temporal_attn.out_proj.weight', 'model.image_tower.image_tower.encoder.layers.15.self_attn.v_proj.weight', 'model.image_tower.image_tower.encoder.layers.6.self_attn.out_proj.bias', 'model.video_tower.video_tower.encoder.layers.11.temporal_attn.out_proj.weight', 'model.video_tower.video_tower.encoder.layers.9.self_attn.k_proj.weight', 'model.video_tower.video_tower.encoder.layers.9.self_attn.v_proj.bias', 'model.video_tower.video_tower.encoder.layers.17.temporal_layer_norm1.weight', 'model.video_tower.video_tower.encoder.layers.9.self_attn.v_proj.weight', 'model.video_tower.video_tower.encoder.layers.13.self_attn.k_proj.weight', 'model.video_tower.video_tower.post_layernorm.weight', 'model.image_tower.image_tower.encoder.layers.13.self_attn.v_proj.bias', 'model.video_tower.video_tower.encoder.layers.12.temporal_attn.v_proj.bias', 'model.image_tower.image_tower.encoder.layers.17.self_attn.v_proj.bias', 'model.video_tower.video_tower.encoder.layers.0.self_attn.out_proj.bias', 'model.video_tower.video_tower.encoder.layers.8.mlp.fc2.weight', 'model.image_tower.image_tower.encoder.layers.19.layer_norm2.bias', 'model.video_tower.video_tower.encoder.layers.10.temporal_attn.out_proj.weight', 'model.video_tower.video_tower.encoder.layers.5.self_attn.out_proj.bias', 'model.image_tower.image_tower.encoder.layers.18.self_attn.k_proj.weight', 'model.video_tower.video_tower.encoder.layers.6.layer_norm1.bias', 'model.video_tower.video_tower.encoder.layers.0.temporal_attn.v_proj.weight', 'model.video_tower.video_tower.encoder.layers.22.self_attn.out_proj.weight', 'model.video_tower.video_tower.encoder.layers.9.self_attn.q_proj.bias', 'model.image_tower.image_tower.encoder.layers.18.mlp.fc2.weight', 'model.image_tower.image_tower.encoder.layers.13.mlp.fc2.weight', 'model.video_tower.video_tower.encoder.layers.20.temporal_attn.v_proj.weight', 'model.video_tower.video_tower.embeddings.position_embedding.weight', 'model.image_tower.image_tower.encoder.layers.12.mlp.fc2.bias', 'model.video_tower.video_tower.encoder.layers.17.mlp.fc1.bias', 'model.video_tower.video_tower.encoder.layers.20.self_attn.k_proj.bias', 'model.image_tower.image_tower.encoder.layers.19.self_attn.v_proj.weight', 'model.video_tower.video_tower.encoder.layers.9.temporal_layer_norm1.bias', 'model.video_tower.video_tower.encoder.layers.18.temporal_attn.k_proj.weight', 'model.video_tower.video_tower.encoder.layers.16.temporal_attn.k_proj.bias', 'model.video_tower.video_tower.encoder.layers.11.self_attn.out_proj.bias', 'model.image_tower.image_tower.encoder.layers.1.self_attn.v_proj.weight', 'model.video_tower.video_tower.encoder.layers.21.self_attn.v_proj.weight', 'model.video_tower.video_tower.encoder.layers.10.temporal_attn.q_proj.bias', 'model.video_tower.video_tower.encoder.layers.13.mlp.fc1.bias', 'model.video_tower.video_tower.encoder.layers.18.temporal_attn.out_proj.bias', 'model.image_tower.image_tower.encoder.layers.0.self_attn.q_proj.bias', 'model.image_tower.image_tower.encoder.layers.12.mlp.fc1.weight', 'model.image_tower.image_tower.encoder.layers.16.self_attn.out_proj.bias', 'model.image_tower.image_tower.encoder.layers.11.mlp.fc1.bias', 'model.image_tower.image_tower.encoder.layers.9.mlp.fc2.bias', 'model.video_tower.video_tower.encoder.layers.6.self_attn.out_proj.weight', 'model.video_tower.video_tower.encoder.layers.3.self_attn.v_proj.bias', 'model.image_tower.image_tower.encoder.layers.14.self_attn.k_proj.weight', 'model.image_tower.image_tower.encoder.layers.22.mlp.fc2.bias', 'model.image_tower.image_tower.encoder.layers.22.self_attn.q_proj.bias', 'model.image_tower.image_tower.encoder.layers.3.self_attn.out_proj.weight', 'model.video_tower.video_tower.encoder.layers.1.self_attn.v_proj.weight', 'model.video_tower.video_tower.encoder.layers.21.temporal_embedding', 'model.image_tower.image_tower.encoder.layers.15.layer_norm2.bias', 'model.image_tower.image_tower.encoder.layers.1.layer_norm2.bias', 'model.video_tower.video_tower.encoder.layers.11.temporal_attn.k_proj.bias', 'model.video_tower.video_tower.encoder.layers.19.layer_norm1.bias', 'model.image_tower.image_tower.encoder.layers.22.mlp.fc2.weight', 'model.image_tower.image_tower.encoder.layers.14.self_attn.k_proj.bias', 'model.video_tower.video_tower.encoder.layers.5.temporal_attn.out_proj.bias', 'model.video_tower.video_tower.encoder.layers.13.self_attn.k_proj.bias', 'model.image_tower.image_tower.encoder.layers.2.layer_norm1.bias', 'model.video_tower.video_tower.encoder.layers.7.temporal_attn.k_proj.bias', 'model.video_tower.video_tower.encoder.layers.8.self_attn.out_proj.weight', 'model.video_tower.video_tower.encoder.layers.15.layer_norm1.weight', 'model.image_tower.image_tower.pre_layrnorm.weight', 'model.video_tower.video_tower.encoder.layers.18.self_attn.q_proj.bias', 'model.video_tower.video_tower.encoder.layers.4.temporal_attn.q_proj.bias', 'model.video_tower.video_tower.encoder.layers.5.self_attn.q_proj.bias', 'model.video_tower.video_tower.encoder.layers.11.self_attn.v_proj.bias', 'model.video_tower.video_tower.encoder.layers.9.temporal_attn.q_proj.bias', 'model.image_tower.image_tower.encoder.layers.6.self_attn.q_proj.weight', 'model.video_tower.video_tower.encoder.layers.0.self_attn.q_proj.weight', 'model.video_tower.video_tower.encoder.layers.11.self_attn.out_proj.weight', 'model.video_tower.video_tower.encoder.layers.22.temporal_attn.k_proj.bias', 'model.video_tower.video_tower.encoder.layers.19.temporal_attn.q_proj.bias', 'model.video_tower.video_tower.encoder.layers.1.temporal_attn.out_proj.weight', 'model.video_tower.video_tower.encoder.layers.5.temporal_embedding', 'model.video_tower.video_tower.encoder.layers.8.temporal_embedding', 'model.video_tower.video_tower.encoder.layers.18.self_attn.v_proj.bias', 'model.video_tower.video_tower.encoder.layers.11.layer_norm1.bias', 'model.video_tower.video_tower.encoder.layers.17.temporal_embedding', 'model.image_tower.image_tower.encoder.layers.8.mlp.fc1.weight', 'model.video_tower.video_tower.encoder.layers.0.mlp.fc1.weight', 'model.image_tower.image_tower.encoder.layers.19.self_attn.out_proj.weight', 'model.image_tower.image_tower.encoder.layers.12.self_attn.k_proj.bias', 'model.image_tower.image_tower.encoder.layers.21.self_attn.k_proj.bias', 'model.image_tower.image_tower.encoder.layers.22.layer_norm1.bias', 'model.video_tower.video_tower.encoder.layers.17.layer_norm1.weight', 'model.video_tower.video_tower.encoder.layers.2.temporal_embedding', 'model.image_tower.image_tower.encoder.layers.2.self_attn.out_proj.bias', 'model.video_tower.video_tower.encoder.layers.2.self_attn.q_proj.bias', 'model.image_tower.image_tower.encoder.layers.15.mlp.fc2.weight', 'model.image_tower.image_tower.encoder.layers.14.layer_norm1.weight', 'model.video_tower.video_tower.encoder.layers.1.layer_norm2.weight', 'model.video_tower.video_tower.encoder.layers.10.temporal_embedding', 'model.video_tower.video_tower.encoder.layers.10.self_attn.out_proj.bias', 'model.video_tower.video_tower.encoder.layers.23.layer_norm2.bias', 'model.image_tower.image_tower.encoder.layers.11.layer_norm2.bias', 'model.video_tower.video_tower.encoder.layers.14.layer_norm1.weight', 'model.image_tower.image_tower.encoder.layers.17.self_attn.out_proj.bias', 'model.video_tower.video_tower.encoder.layers.20.layer_norm1.weight', 'model.image_tower.image_tower.encoder.layers.9.mlp.fc1.bias', 'model.image_tower.image_tower.encoder.layers.2.self_attn.out_proj.weight', 'model.image_tower.image_tower.encoder.layers.10.mlp.fc2.weight', 'model.video_tower.video_tower.encoder.layers.18.layer_norm1.weight', 'model.image_tower.image_tower.encoder.layers.11.self_attn.out_proj.bias', 'model.image_tower.image_tower.encoder.layers.1.mlp.fc1.bias', 'model.video_tower.video_tower.encoder.layers.22.temporal_attn.out_proj.bias', 'model.video_tower.video_tower.encoder.layers.10.self_attn.k_proj.bias', 'model.image_tower.image_tower.encoder.layers.6.mlp.fc2.weight', 'model.video_tower.video_tower.encoder.layers.3.self_attn.v_proj.weight', 'model.video_tower.video_tower.encoder.layers.11.mlp.fc2.bias', 'model.image_tower.image_tower.encoder.layers.7.mlp.fc2.weight', 'model.video_tower.video_tower.encoder.layers.1.mlp.fc1.weight', 'model.video_tower.video_tower.encoder.layers.1.self_attn.k_proj.weight', 'model.video_tower.video_tower.encoder.layers.10.mlp.fc2.bias', 'model.video_tower.video_tower.encoder.layers.11.self_attn.q_proj.weight', 'model.image_tower.image_tower.encoder.layers.20.layer_norm1.bias', 'model.video_tower.video_tower.encoder.layers.21.temporal_attn.k_proj.bias', 'model.video_tower.video_tower.encoder.layers.3.mlp.fc1.weight', 'model.video_tower.video_tower.encoder.layers.15.temporal_attn.out_proj.bias', 'model.video_tower.video_tower.encoder.layers.22.self_attn.v_proj.bias', 'model.video_tower.video_tower.encoder.layers.9.temporal_embedding', 'model.video_tower.video_tower.encoder.layers.11.temporal_embedding', 'model.video_tower.video_tower.encoder.layers.21.self_attn.out_proj.bias', 'model.video_tower.video_tower.encoder.layers.3.temporal_layer_norm1.bias', 'model.video_tower.video_tower.encoder.layers.19.mlp.fc2.bias', 'model.image_tower.image_tower.encoder.layers.13.self_attn.out_proj.weight', 'model.video_tower.video_tower.encoder.layers.6.mlp.fc2.bias', 'model.video_tower.video_tower.encoder.layers.6.layer_norm2.bias', 'model.video_tower.video_tower.encoder.layers.9.temporal_attn.v_proj.bias', 'model.video_tower.video_tower.encoder.layers.16.temporal_attn.q_proj.weight', 'model.video_tower.video_tower.encoder.layers.18.layer_norm1.bias', 'model.video_tower.video_tower.encoder.layers.8.temporal_attn.k_proj.weight', 'model.video_tower.video_tower.encoder.layers.10.self_attn.v_proj.bias', 'model.video_tower.video_tower.encoder.layers.2.temporal_attn.v_proj.weight', 'model.image_tower.image_tower.encoder.layers.6.self_attn.k_proj.weight', 'model.image_tower.image_tower.encoder.layers.6.layer_norm1.weight', 'model.video_tower.video_tower.encoder.layers.21.temporal_attn.v_proj.weight', 'model.video_tower.video_tower.encoder.layers.10.mlp.fc2.weight', 'model.video_tower.video_tower.encoder.layers.11.layer_norm2.bias', 'model.video_tower.video_tower.encoder.layers.3.temporal_attn.k_proj.weight', 'model.video_tower.video_tower.encoder.layers.6.self_attn.out_proj.bias', 'model.video_tower.video_tower.encoder.layers.6.self_attn.v_proj.bias', 'model.video_tower.video_tower.encoder.layers.9.temporal_attn.k_proj.weight', 'model.video_tower.video_tower.encoder.layers.7.temporal_attn.k_proj.weight', 'model.image_tower.image_tower.encoder.layers.12.self_attn.q_proj.bias', 'model.image_tower.image_tower.encoder.layers.5.layer_norm1.bias', 'model.video_tower.video_tower.encoder.layers.2.temporal_attn.v_proj.bias', 'model.image_tower.image_tower.encoder.layers.0.layer_norm1.weight', 'model.video_tower.video_tower.encoder.layers.1.layer_norm1.weight', 'model.video_tower.video_tower.encoder.layers.16.self_attn.v_proj.weight', 'model.video_tower.video_tower.encoder.layers.7.self_attn.v_proj.weight', 'model.image_tower.image_tower.encoder.layers.3.mlp.fc2.bias', 'model.video_tower.video_tower.encoder.layers.16.temporal_attn.k_proj.weight', 'model.video_tower.video_tower.encoder.layers.19.self_attn.k_proj.bias', 'model.video_tower.video_tower.encoder.layers.22.self_attn.k_proj.weight', 'model.video_tower.video_tower.encoder.layers.6.self_attn.v_proj.weight', 'model.video_tower.video_tower.encoder.layers.12.self_attn.k_proj.weight', 'model.video_tower.video_tower.encoder.layers.23.mlp.fc1.bias', 'model.video_tower.video_tower.encoder.layers.8.self_attn.v_proj.bias', 'model.video_tower.video_tower.encoder.layers.9.temporal_attn.out_proj.bias', 'model.video_tower.video_tower.encoder.layers.20.layer_norm1.bias', 'model.video_tower.video_tower.encoder.layers.14.temporal_attn.out_proj.bias', 'model.video_tower.video_tower.encoder.layers.9.temporal_attn.q_proj.weight', 'model.video_tower.video_tower.encoder.layers.2.self_attn.v_proj.bias', 'model.image_tower.image_tower.encoder.layers.8.self_attn.k_proj.bias', 'model.video_tower.video_tower.encoder.layers.0.layer_norm2.bias', 'model.video_tower.video_tower.encoder.layers.11.mlp.fc1.weight', 'model.video_tower.video_tower.encoder.layers.13.temporal_attn.k_proj.bias', 'model.image_tower.image_tower.encoder.layers.23.mlp.fc2.bias', 'model.image_tower.image_tower.encoder.layers.2.mlp.fc1.weight', 'model.video_tower.video_tower.encoder.layers.12.self_attn.q_proj.bias', 'model.image_tower.image_tower.encoder.layers.17.mlp.fc1.bias', 'model.image_tower.image_tower.encoder.layers.11.self_attn.k_proj.weight', 'model.video_tower.video_tower.encoder.layers.10.temporal_attn.k_proj.bias', 'model.video_tower.video_tower.encoder.layers.20.temporal_embedding', 'model.video_tower.video_tower.encoder.layers.17.self_attn.v_proj.bias', 'model.image_tower.image_tower.encoder.layers.2.self_attn.v_proj.weight', 'model.image_tower.image_tower.encoder.layers.18.layer_norm2.weight', 'model.video_tower.video_tower.encoder.layers.10.self_attn.k_proj.weight', 'model.video_tower.video_tower.encoder.layers.16.self_attn.out_proj.weight', 'model.video_tower.video_tower.encoder.layers.12.self_attn.v_proj.weight', 'model.image_tower.image_tower.encoder.layers.21.self_attn.v_proj.weight', 'model.video_tower.video_tower.encoder.layers.0.temporal_attn.k_proj.weight', 'model.image_tower.image_tower.encoder.layers.19.layer_norm1.bias', 'model.video_tower.video_tower.encoder.layers.19.temporal_layer_norm1.weight', 'model.video_tower.video_tower.encoder.layers.3.temporal_embedding', 'model.video_tower.video_tower.encoder.layers.12.self_attn.out_proj.weight', 'model.image_tower.image_tower.encoder.layers.22.self_attn.out_proj.bias', 'model.video_tower.video_tower.encoder.layers.13.mlp.fc2.bias', 'model.image_tower.image_tower.encoder.layers.0.layer_norm2.weight', 'model.image_tower.image_tower.encoder.layers.11.self_attn.k_proj.bias', 'model.video_tower.video_tower.encoder.layers.10.self_attn.q_proj.bias', 'model.video_tower.video_tower.encoder.layers.17.temporal_attn.q_proj.bias', 'model.video_tower.video_tower.encoder.layers.16.self_attn.k_proj.weight', 'model.image_tower.image_tower.encoder.layers.0.self_attn.q_proj.weight', 'model.image_tower.image_tower.encoder.layers.1.mlp.fc1.weight', 'model.video_tower.video_tower.encoder.layers.8.self_attn.out_proj.bias', 'model.video_tower.video_tower.encoder.layers.1.self_attn.k_proj.bias', 'model.video_tower.video_tower.encoder.layers.6.temporal_embedding', 'model.video_tower.video_tower.encoder.layers.13.layer_norm2.bias', 'model.video_tower.video_tower.encoder.layers.15.layer_norm2.bias', 'model.video_tower.video_tower.encoder.layers.7.temporal_attn.v_proj.bias', 'model.image_tower.image_tower.encoder.layers.15.self_attn.out_proj.weight', 'model.image_tower.image_tower.encoder.layers.21.mlp.fc1.bias', 'model.video_tower.video_tower.encoder.layers.10.temporal_layer_norm1.bias', 'model.video_tower.video_tower.encoder.layers.5.temporal_attn.out_proj.weight', 'model.image_tower.image_tower.encoder.layers.23.mlp.fc1.bias', 'model.image_tower.image_tower.encoder.layers.1.self_attn.out_proj.bias', 'model.video_tower.video_tower.encoder.layers.5.layer_norm2.bias', 'model.video_tower.video_tower.encoder.layers.16.temporal_embedding', 'model.image_tower.image_tower.encoder.layers.1.self_attn.q_proj.weight', 'model.video_tower.video_tower.encoder.layers.18.temporal_attn.out_proj.weight', 'model.image_tower.image_tower.encoder.layers.8.self_attn.q_proj.bias', 'model.video_tower.video_tower.encoder.layers.13.layer_norm1.weight', 'model.video_tower.video_tower.encoder.layers.7.mlp.fc2.bias', 'model.video_tower.video_tower.encoder.layers.2.temporal_attn.k_proj.bias', 'model.video_tower.video_tower.encoder.layers.18.temporal_layer_norm1.weight', 'model.image_tower.image_tower.encoder.layers.11.self_attn.q_proj.bias', 'model.image_tower.image_tower.post_layernorm.weight', 'model.video_tower.video_tower.encoder.layers.19.temporal_attn.k_proj.bias', 'model.image_tower.image_tower.encoder.layers.10.layer_norm1.bias', 'model.video_tower.video_tower.encoder.layers.0.layer_norm1.weight', 'model.video_tower.video_tower.encoder.layers.11.self_attn.k_proj.bias', 'model.video_tower.video_tower.encoder.layers.18.mlp.fc2.bias', 'model.video_tower.video_tower.encoder.layers.13.self_attn.v_proj.bias', 'model.image_tower.image_tower.encoder.layers.0.self_attn.v_proj.bias', 'model.image_tower.image_tower.encoder.layers.8.layer_norm1.weight', 'model.image_tower.image_tower.encoder.layers.7.self_attn.k_proj.bias', 'model.video_tower.video_tower.encoder.layers.22.layer_norm2.weight', 'model.image_tower.image_tower.encoder.layers.22.mlp.fc1.bias', 'model.video_tower.video_tower.encoder.layers.7.self_attn.k_proj.bias', 'model.image_tower.image_tower.encoder.layers.6.layer_norm2.weight', 'model.video_tower.video_tower.encoder.layers.7.mlp.fc2.weight', 'model.video_tower.video_tower.encoder.layers.7.layer_norm1.weight', 'model.video_tower.video_tower.encoder.layers.8.temporal_attn.out_proj.weight', 'model.video_tower.video_tower.encoder.layers.6.layer_norm1.weight', 'model.video_tower.video_tower.encoder.layers.18.mlp.fc1.weight', 'model.video_tower.video_tower.encoder.layers.20.mlp.fc2.bias', 'model.video_tower.video_tower.encoder.layers.13.temporal_embedding', 'model.video_tower.video_tower.encoder.layers.12.self_attn.out_proj.bias', 'model.video_tower.video_tower.encoder.layers.3.mlp.fc2.weight', 'model.image_tower.image_tower.encoder.layers.5.self_attn.v_proj.weight', 'model.image_tower.image_tower.encoder.layers.18.layer_norm1.bias', 'model.image_tower.image_tower.encoder.layers.21.layer_norm2.bias', 'model.video_tower.video_tower.encoder.layers.20.temporal_attn.k_proj.bias', 'model.video_tower.video_tower.encoder.layers.10.layer_norm2.bias', 'model.image_tower.image_tower.encoder.layers.21.self_attn.v_proj.bias', 'model.video_tower.video_tower.encoder.layers.22.self_attn.q_proj.bias', 'model.video_tower.video_tower.encoder.layers.6.temporal_attn.k_proj.weight', 'model.video_tower.video_tower.encoder.layers.6.temporal_attn.v_proj.bias', 'model.image_tower.image_tower.encoder.layers.22.mlp.fc1.weight', 'model.video_tower.video_tower.encoder.layers.5.temporal_layer_norm1.bias', 'model.video_tower.video_tower.encoder.layers.23.self_attn.k_proj.weight', 'model.image_tower.image_tower.encoder.layers.6.self_attn.q_proj.bias', 'model.video_tower.video_tower.encoder.layers.14.temporal_attn.q_proj.weight', 'model.image_tower.image_tower.encoder.layers.23.layer_norm2.weight', 'model.video_tower.video_tower.encoder.layers.11.temporal_attn.q_proj.bias', 'model.video_tower.video_tower.encoder.layers.10.mlp.fc1.weight', 'model.image_tower.image_tower.encoder.layers.1.self_attn.v_proj.bias', 'model.video_tower.video_tower.encoder.layers.17.self_attn.out_proj.weight', 'model.video_tower.video_tower.encoder.layers.7.temporal_attn.v_proj.weight', 'model.video_tower.video_tower.encoder.layers.8.temporal_attn.k_proj.bias', 'model.image_tower.image_tower.encoder.layers.11.layer_norm1.bias', 'model.video_tower.video_tower.encoder.layers.19.temporal_attn.out_proj.bias', 'model.video_tower.video_tower.encoder.layers.13.mlp.fc2.weight', 'model.video_tower.video_tower.encoder.layers.19.temporal_attn.q_proj.weight', 'model.video_tower.video_tower.encoder.layers.12.layer_norm2.bias', 'model.video_tower.video_tower.encoder.layers.3.self_attn.k_proj.weight', 'model.video_tower.video_tower.encoder.layers.14.self_attn.out_proj.weight', 'model.video_tower.video_tower.encoder.layers.1.temporal_attn.out_proj.bias', 'model.video_tower.video_tower.encoder.layers.16.self_attn.v_proj.bias', 'model.video_tower.video_tower.encoder.layers.12.temporal_layer_norm1.weight', 'model.image_tower.image_tower.encoder.layers.8.self_attn.v_proj.weight', 'model.image_tower.image_tower.encoder.layers.7.mlp.fc2.bias', 'model.image_tower.image_tower.encoder.layers.5.mlp.fc2.bias', 'model.image_tower.image_tower.encoder.layers.21.self_attn.k_proj.weight', 'model.video_tower.video_tower.encoder.layers.18.self_attn.v_proj.weight', 'model.image_tower.image_tower.encoder.layers.3.self_attn.k_proj.bias', 'model.image_tower.image_tower.encoder.layers.8.mlp.fc2.bias', 'model.video_tower.video_tower.encoder.layers.15.layer_norm2.weight', 'model.video_tower.video_tower.encoder.layers.2.temporal_layer_norm1.bias', 'model.video_tower.video_tower.encoder.layers.13.temporal_layer_norm1.bias', 'model.video_tower.video_tower.encoder.layers.13.self_attn.out_proj.bias', 'model.video_tower.video_tower.encoder.layers.14.temporal_attn.out_proj.weight', 'model.image_tower.image_tower.embeddings.class_embedding', 'model.image_tower.image_tower.encoder.layers.18.self_attn.out_proj.weight', 'model.video_tower.video_tower.encoder.layers.10.temporal_attn.v_proj.bias', 'model.video_tower.video_tower.encoder.layers.20.temporal_attn.q_proj.bias', 'model.video_tower.video_tower.encoder.layers.2.mlp.fc1.weight', 'model.image_tower.image_tower.encoder.layers.4.mlp.fc2.bias', 'model.video_tower.video_tower.encoder.layers.14.temporal_attn.k_proj.weight', 'model.video_tower.video_tower.encoder.layers.5.layer_norm1.weight', 'model.video_tower.video_tower.encoder.layers.0.mlp.fc2.bias', 'model.video_tower.video_tower.encoder.layers.9.mlp.fc1.bias', 'model.video_tower.video_tower.encoder.layers.3.layer_norm2.bias', 'model.image_tower.image_tower.encoder.layers.6.mlp.fc1.weight', 'model.video_tower.video_tower.encoder.layers.0.layer_norm1.bias', 'model.video_tower.video_tower.encoder.layers.14.temporal_attn.v_proj.weight', 'model.image_tower.image_tower.encoder.layers.18.self_attn.q_proj.bias', 'model.video_tower.video_tower.encoder.layers.10.temporal_attn.v_proj.weight']\n",
      "- This IS expected if you are initializing LlavaLlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LlavaLlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLaVA結束載入\n"
     ]
    }
   ],
   "source": [
    "print(\"LLaVA開始\")\n",
    "disable_torch_init()\n",
    "model_path = 'LanguageBind/Video-LLaVA-7B'\n",
    "cache_dir = 'cache_dir'\n",
    "device = 'cuda'\n",
    "load_4bit, load_8bit = True, False\n",
    "model_name = 'Video-LLaVA'\n",
    "print(\"LLaVA載入模型\") \n",
    "tokenizer, LLaVA_model, processor, _ = load_pretrained_model(model_path, None, model_name, load_8bit, load_4bit, device=device, cache_dir=cache_dir)\n",
    "print(\"LLaVA結束載入\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc1d4d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in tower_videos.shape torch.Size([16, 3, 224, 224])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (257) must match the size of tensor b (577) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[49], line 6\u001b[0m\n",
      "\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#print(\"你 prompt 中的 <image> token 數量 =\", num_image_tokens)\u001b[39;00m\n",
      "\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#print(projected.shape)\u001b[39;00m\n",
      "\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#print(list[projected])\u001b[39;00m\n",
      "\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39minference_mode():\n",
      "\u001b[0;32m----> 6\u001b[0m     output_ids \u001b[38;5;241m=\u001b[39m \u001b[43mLLaVA_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvideo_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n",
      "\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m     16\u001b[0m output_text \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(output_ids[\u001b[38;5;241m0\u001b[39m, input_ids\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]:], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLLaVA 回答：\u001b[39m\u001b[38;5;124m\"\u001b[39m, output_text)\n",
      "\n",
      "File \u001b[0;32m~/anaconda3/envs/videollava/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n",
      "\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n",
      "\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n",
      "\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/anaconda3/envs/videollava/lib/python3.10/site-packages/transformers/generation/utils.py:1588\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1580\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n",
      "\u001b[1;32m   1581\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n",
      "\u001b[1;32m   1582\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n",
      "\u001b[1;32m   1583\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n",
      "\u001b[1;32m   1584\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n",
      "\u001b[1;32m   1585\u001b[0m     )\n",
      "\u001b[1;32m   1587\u001b[0m     \u001b[38;5;66;03m# 13. run sample\u001b[39;00m\n",
      "\u001b[0;32m-> 1588\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m   1589\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1590\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1591\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_warper\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1592\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1593\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1594\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1595\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1596\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1597\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1598\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1599\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   1600\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1602\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_beam_gen_mode:\n",
      "\u001b[1;32m   1603\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m generation_config\u001b[38;5;241m.\u001b[39mnum_return_sequences \u001b[38;5;241m>\u001b[39m generation_config\u001b[38;5;241m.\u001b[39mnum_beams:\n",
      "\n",
      "File \u001b[0;32m~/anaconda3/envs/videollava/lib/python3.10/site-packages/transformers/generation/utils.py:2642\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n",
      "\u001b[1;32m   2639\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n",
      "\u001b[1;32m   2641\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n",
      "\u001b[0;32m-> 2642\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m   2643\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   2644\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   2645\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   2646\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m   2647\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   2649\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n",
      "\u001b[1;32m   2650\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m~/anaconda3/envs/videollava/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n",
      "\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n",
      "\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "\n",
      "File \u001b[0;32m~/anaconda3/envs/videollava/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mold_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "\n",
      "File \u001b[0;32m~/xclip/Video-LLaVA/videollava/model/language_model/llava_llama.py:79\u001b[0m, in \u001b[0;36mLlavaLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, images, return_dict)\u001b[0m\n",
      "\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n",
      "\u001b[1;32m     57\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n",
      "\u001b[1;32m     58\u001b[0m     input_ids: torch\u001b[38;5;241m.\u001b[39mLongTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m     68\u001b[0m     return_dict: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "\u001b[1;32m     69\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tuple, CausalLMOutputWithPast]:\n",
      "\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;32m     72\u001b[0m         (\n",
      "\u001b[1;32m     73\u001b[0m             input_ids,\n",
      "\u001b[1;32m     74\u001b[0m             position_ids,\n",
      "\u001b[1;32m     75\u001b[0m             attention_mask,\n",
      "\u001b[1;32m     76\u001b[0m             past_key_values,\n",
      "\u001b[1;32m     77\u001b[0m             inputs_embeds,\n",
      "\u001b[1;32m     78\u001b[0m             labels\n",
      "\u001b[0;32m---> 79\u001b[0m         ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_inputs_labels_for_multimodal\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m     80\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     81\u001b[0m \u001b[43m            \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     82\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     83\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     84\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     85\u001b[0m \u001b[43m            \u001b[49m\u001b[43mimages\u001b[49m\n",
      "\u001b[1;32m     86\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mforward(\n",
      "\u001b[1;32m     89\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n",
      "\u001b[1;32m     90\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m     98\u001b[0m         return_dict\u001b[38;5;241m=\u001b[39mreturn_dict\n",
      "\u001b[1;32m     99\u001b[0m     )\n",
      "\n",
      "File \u001b[0;32m~/xclip/Video-LLaVA/videollava/model/llava_arch.py:219\u001b[0m, in \u001b[0;36mLlavaMetaForCausalLM.prepare_inputs_labels_for_multimodal\u001b[0;34m(self, input_ids, position_ids, attention_mask, past_key_values, labels, images)\u001b[0m\n",
      "\u001b[1;32m    216\u001b[0m         tmp_image_features[pos] \u001b[38;5;241m=\u001b[39m image_features_minibatch[i]\n",
      "\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(videos_minibatch, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mndim\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m5\u001b[39m:  \u001b[38;5;66;03m# batch consists of videos, [mini_b, c, t, h, w]\u001b[39;00m\n",
      "\u001b[0;32m--> 219\u001b[0m     video_features_minibatch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_videos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideos_minibatch\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# fake list [mini_b, t, l, c]\u001b[39;00m\n",
      "\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, pos \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(video_idx):\n",
      "\u001b[1;32m    221\u001b[0m         t \u001b[38;5;241m=\u001b[39m video_features_minibatch[i]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "\n",
      "File \u001b[0;32m~/xclip/Video-LLaVA/videollava/model/llava_arch.py:146\u001b[0m, in \u001b[0;36mLlavaMetaForCausalLM.encode_videos\u001b[0;34m(self, videos)\u001b[0m\n",
      "\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mencode_videos\u001b[39m(\u001b[38;5;28mself\u001b[39m, videos):  \u001b[38;5;66;03m# [mini_b, c, t, h, w]\u001b[39;00m\n",
      "\u001b[1;32m    145\u001b[0m     b, _, t, _, _ \u001b[38;5;241m=\u001b[39m videos\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;32m--> 146\u001b[0m     video_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_video_tower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideos\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [mini_b, t, n, c]\u001b[39;00m\n",
      "\u001b[1;32m    147\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min encode_videos video_features=\u001b[39m\u001b[38;5;124m\"\u001b[39m,video_features)\n",
      "\u001b[1;32m    148\u001b[0m     video_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_model()\u001b[38;5;241m.\u001b[39mmm_projector(video_features)\n",
      "\n",
      "File \u001b[0;32m~/anaconda3/envs/videollava/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n",
      "\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n",
      "\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "\n",
      "Cell \u001b[0;32mIn[33], line 39\u001b[0m, in \u001b[0;36mXCLIPVideoTower.forward\u001b[0;34m(self, videos, **kwargs)\u001b[0m\n",
      "\u001b[1;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min tower_videos.shape\u001b[39m\u001b[38;5;124m\"\u001b[39m, videos\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# 前向传播\u001b[39;00m\n",
      "\u001b[0;32m---> 39\u001b[0m video_forward_outs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvideo_tower\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvision_model\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvideos\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n",
      "\u001b[1;32m     42\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# 重塑输出 [B*T, L, C] -> [B, T, L, C]\u001b[39;00m\n",
      "\u001b[1;32m     45\u001b[0m video_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_select(video_forward_outs)\n",
      "\n",
      "File \u001b[0;32m~/anaconda3/envs/videollava/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n",
      "\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n",
      "\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "\n",
      "File \u001b[0;32m~/anaconda3/envs/videollava/lib/python3.10/site-packages/transformers/models/x_clip/modeling_x_clip.py:1022\u001b[0m, in \u001b[0;36mXCLIPVisionTransformer.forward\u001b[0;34m(self, pixel_values, output_attentions, output_hidden_states, return_dict)\u001b[0m\n",
      "\u001b[1;32m   1017\u001b[0m output_hidden_states \u001b[38;5;241m=\u001b[39m (\n",
      "\u001b[1;32m   1018\u001b[0m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_hidden_states\n",
      "\u001b[1;32m   1019\u001b[0m )\n",
      "\u001b[1;32m   1020\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n",
      "\u001b[0;32m-> 1022\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1023\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_layernorm(hidden_states)\n",
      "\u001b[1;32m   1025\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n",
      "\u001b[1;32m   1026\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39mhidden_states,\n",
      "\u001b[1;32m   1027\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n",
      "\u001b[1;32m   1028\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n",
      "\u001b[1;32m   1029\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n",
      "\u001b[1;32m   1030\u001b[0m )\n",
      "\n",
      "File \u001b[0;32m~/anaconda3/envs/videollava/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n",
      "\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n",
      "\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "\n",
      "File \u001b[0;32m~/anaconda3/envs/videollava/lib/python3.10/site-packages/transformers/models/x_clip/modeling_x_clip.py:151\u001b[0m, in \u001b[0;36mXCLIPVisionEmbeddings.forward\u001b[0;34m(self, pixel_values)\u001b[0m\n",
      "\u001b[1;32m    149\u001b[0m class_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_embedding\u001b[38;5;241m.\u001b[39mexpand(batch_size, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;32m    150\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([class_embeds, patch_embeds], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;32m--> 151\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[43membeddings\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mposition_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\n",
      "\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (257) must match the size of tensor b (577) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "num_image_tokens = (input_ids == IMAGE_TOKEN_INDEX).sum().item()\n",
    "#print(\"你 prompt 中的 <image> token 數量 =\", num_image_tokens)\n",
    "#print(projected.shape)\n",
    "#print(list[projected])\n",
    "with torch.inference_mode():\n",
    "    output_ids = LLaVA_model.generate(\n",
    "        input_ids=input_ids,\n",
    "        images=video_tensor,  \n",
    "        do_sample=True,\n",
    "        temperature=0.1,\n",
    "        max_new_tokens=1024,\n",
    "        use_cache=True,\n",
    "        stopping_criteria=[stopping_criteria],\n",
    "    )\n",
    "\n",
    "output_text = tokenizer.decode(output_ids[0, input_ids.shape[1]:], skip_special_tokens=True)\n",
    "print(\"LLaVA 回答：\", output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44c09f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear4bit(in_features=1024, out_features=4096, bias=True)\n"
     ]
    }
   ],
   "source": [
    "print(LLaVA_model.model.mm_projector[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7d1b728",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"This is a sign language video. Please translate it into English.\"\n",
    "conv = conv_templates[\"llava_v1\"].copy()\n",
    "conv.messages = []  \n",
    "conv.append_message(conv.roles[0], DEFAULT_IMAGE_TOKEN + '\\n' + prompt)\n",
    "conv.append_message(conv.roles[1], None)\n",
    "prompt = conv.get_prompt()\n",
    "input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).cuda()\n",
    "stop_str = conv.sep\n",
    "keywords = [stop_str]\n",
    "stopping_criteria = KeywordsStoppingCriteria(keywords, tokenizer, input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ea211408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 16, 336, 336])\n"
     ]
    }
   ],
   "source": [
    "video_tensor = (torch.rand(1, 3, 16, 336, 336) * 255).clamp(0, 255).float().to(\"cuda\")\n",
    "print(video_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fd4ca29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import XCLIPModel, XCLIPProcessor, CLIPVisionConfig\n",
    "class XCLIPVideoTower(nn.Module):\n",
    "    def __init__(self, video_tower_path, args=None, delay_load=False):\n",
    "        super().__init__()\n",
    "        self.is_loaded = False if delay_load else True\n",
    "        self.video_tower_path = video_tower_path\n",
    "        \n",
    "        if not delay_load:\n",
    "            # 直接加载模型\n",
    "            self.video_tower = XCLIPModel.from_pretrained(video_tower_path).half().cuda()\n",
    "            self.select_layer = -2\n",
    "            self.select_feature = 'patch'\n",
    "            \n",
    "    def load_model(self):\n",
    "        if not self.is_loaded:\n",
    "            self.video_tower = XCLIPModel.from_pretrained(self.video_tower_path).half().cuda()\n",
    "            self.is_loaded = True\n",
    "            \n",
    "    def feature_select(self, video_forward_outs):\n",
    "        video_features = video_forward_outs.hidden_states[self.select_layer]\n",
    "        if self.select_feature == 'patch':\n",
    "            video_features = video_features[:, 1:]\n",
    "        elif self.select_feature == 'cls_patch':\n",
    "            video_features = video_features\n",
    "        else:\n",
    "            raise ValueError(f'Unexpected select feature: {self.select_feature}')\n",
    "        return video_features\n",
    "        \n",
    "    def forward(self, videos, **kwargs):\n",
    "        if not self.is_loaded:\n",
    "            self.load_model()\n",
    "        videos = videos.half()\n",
    "        # 处理视频维度 [B, C, T, H, W] -> [B*T, C, H, W]\n",
    "        b, c, t, h, w = videos.shape\n",
    "        videos = videos.transpose(1, 2).reshape(-1, c, h, w)\n",
    "        \n",
    "        print(\"in tower_videos.shape\", videos.shape)\n",
    "        # 前向传播\n",
    "        video_forward_outs = self.video_tower.vision_model(\n",
    "            videos,\n",
    "            output_hidden_states=True\n",
    "        )\n",
    "        \n",
    "        # 重塑输出 [B*T, L, C] -> [B, T, L, C]\n",
    "        video_features = self.feature_select(video_forward_outs)\n",
    "        _, l, c = video_features.shape\n",
    "        print(video_features.shape)\n",
    "        video_features = video_features.reshape(b, t, l, c)\n",
    "        \n",
    "        return video_features\n",
    "\n",
    "    @property\n",
    "    def dummy_feature(self):\n",
    "        return torch.zeros(1, self.hidden_size, device=self.device, dtype=self.dtype)\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.video_tower.parameters()).device if self.is_loaded else None\n",
    "\n",
    "    @property\n",
    "    def dtype(self):\n",
    "        return next(self.video_tower.parameters()).dtype if self.is_loaded else None\n",
    "\n",
    "    @property\n",
    "    def config(self):\n",
    "        if self.is_loaded:\n",
    "            return self.video_tower.config\n",
    "        else:\n",
    "            return self.cfg_only\n",
    "\n",
    "    @property\n",
    "    def hidden_size(self):\n",
    "        return self.config.hidden_size\n",
    "\n",
    "    @property\n",
    "    def num_patches(self):\n",
    "        return (self.config.image_size // self.config.patch_size) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "211fc139",
   "metadata": {},
   "outputs": [],
   "source": [
    "llava_video_tower = LLaVA_model.model.video_tower "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "dacc3d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`id2label` is found in both `text_config_dict` and `text_config` but with different values. The value `text_config_dict[\"id2label\"]` will be used instead.\n"
     ]
    }
   ],
   "source": [
    "# 创建参数对象\n",
    "model_args = type('Args', (), {\n",
    "    'video_tower': 'microsoft/xclip-large-patch14-16-frames',\n",
    "    'mm_vision_select_layer': -2,\n",
    "    'mm_vision_select_feature': 'patch',\n",
    "})()\n",
    "# 初始化视频塔\n",
    "video_tower = XCLIPVideoTower(model_args.video_tower, model_args)\n",
    "\n",
    "# 设置到 LLaVA 模型\n",
    "LLaVA_model.model.video_tower = video_tower.half().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aca2d2bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video tower: XCLIPVideoTower(\n",
      "  (video_tower): XCLIPModel(\n",
      "    (text_model): XCLIPTextTransformer(\n",
      "      (embeddings): XCLIPTextEmbeddings(\n",
      "        (token_embedding): Embedding(49408, 768)\n",
      "        (position_embedding): Embedding(77, 768)\n",
      "      )\n",
      "      (encoder): XCLIPEncoder(\n",
      "        (layers): ModuleList(\n",
      "          (0-11): 12 x XCLIPEncoderLayer(\n",
      "            (self_attn): XCLIPAttention(\n",
      "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            )\n",
      "            (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): XCLIPMLP(\n",
      "              (activation_fn): QuickGELUActivation()\n",
      "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            )\n",
      "            (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (vision_model): XCLIPVisionTransformer(\n",
      "      (embeddings): XCLIPVisionEmbeddings(\n",
      "        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
      "        (position_embedding): Embedding(577, 1024)\n",
      "      )\n",
      "      (pre_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (encoder): XCLIPVisionEncoder(\n",
      "        (layers): ModuleList(\n",
      "          (0-23): 24 x XCLIPVisionEncoderLayer(\n",
      "            (message_fc): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (message_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (message_attn): XCLIPAttention(\n",
      "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (drop_path): Identity()\n",
      "            (self_attn): XCLIPAttention(\n",
      "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): XCLIPMLP(\n",
      "              (activation_fn): QuickGELUActivation()\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (visual_projection): Linear(in_features=1024, out_features=768, bias=False)\n",
      "    (text_projection): Linear(in_features=768, out_features=768, bias=False)\n",
      "    (prompts_visual_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (mit): XCLIPMultiframeIntegrationTransformer(\n",
      "      (encoder): XCLIPEncoder(\n",
      "        (layers): ModuleList(\n",
      "          (0): XCLIPEncoderLayer(\n",
      "            (self_attn): XCLIPAttention(\n",
      "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            )\n",
      "            (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): XCLIPMLP(\n",
      "              (activation_fn): QuickGELUActivation()\n",
      "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            )\n",
      "            (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (prompts_generator): XCLIPPromptGenerator(\n",
      "      (layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (decoder): ModuleList(\n",
      "        (0-1): 2 x PromptGeneratorLayer(\n",
      "          (cross_attn): XCLIPCrossAttention(\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Sequential(\n",
      "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (1): QuickGELUActivation()\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "            (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Hidden size: 1024\n",
      "Projector type: mlp2x_gelu\n"
     ]
    }
   ],
   "source": [
    "# 檢查模型配置\n",
    "print(\"Video tower:\",LLaVA_model.get_video_tower())\n",
    "print(\"Hidden size:\", LLaVA_model.config.mm_hidden_size)\n",
    "print(\"Projector type:\", LLaVA_model.config.mm_projector_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "14ca1440",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 16, 336, 336])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "53ae8a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in tower_videos.shape torch.Size([16, 3, 336, 336])\n",
      "torch.Size([16, 576, 1024])\n",
      "in encode_videos video_features= tensor([[[[ 0.3623, -0.7271,  1.0098,  ...,  0.9409,  0.1445,  0.5776],\n",
      "          [-0.0818, -0.8091,  0.9385,  ...,  1.1729,  0.3562,  0.5205],\n",
      "          [-0.6396, -0.9053,  1.2012,  ...,  0.5811, -0.5024, -0.3679],\n",
      "          ...,\n",
      "          [-0.3066, -0.6606,  0.6318,  ...,  0.9624,  0.9106,  0.8086],\n",
      "          [ 0.2788, -0.2385,  1.5967,  ...,  0.3689,  0.7212,  0.1658],\n",
      "          [ 0.3850, -0.7690,  1.1074,  ...,  0.8979,  0.2253,  0.5488]],\n",
      "\n",
      "         [[-0.1271, -1.0342,  1.1426,  ...,  0.6133,  0.5542,  0.9150],\n",
      "          [-0.1790, -0.9780,  0.9219,  ...,  0.9756,  0.2280,  0.8452],\n",
      "          [-0.7939, -0.9902,  0.7412,  ...,  0.3032, -0.0082,  0.2034],\n",
      "          ...,\n",
      "          [-0.1885, -0.4106,  0.9741,  ...,  0.5967,  0.6279,  0.4541],\n",
      "          [ 0.1301, -0.7227,  0.7188,  ...,  0.8760,  0.5317,  0.6445],\n",
      "          [ 0.2417, -0.4995,  0.9238,  ...,  0.6260,  0.0552,  0.7246]],\n",
      "\n",
      "         [[ 0.2219, -0.8926,  1.0352,  ...,  0.8330,  0.4961,  0.9092],\n",
      "          [ 0.1147, -0.8408,  0.8486,  ...,  1.2168,  0.5864,  0.7686],\n",
      "          [-0.5225, -0.8091,  0.7568,  ...,  0.6079, -0.1849,  0.1118],\n",
      "          ...,\n",
      "          [ 0.0054, -0.7637,  0.9932,  ...,  0.8633,  0.5698,  0.3340],\n",
      "          [ 0.3118, -0.0367,  1.7178,  ...,  0.1965,  0.6279,  0.0885],\n",
      "          [ 0.3367, -0.4238,  1.1113,  ...,  0.7520,  0.2119,  0.6450]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.1821, -1.0146,  0.9248,  ...,  0.5107,  0.5068,  0.7769],\n",
      "          [-0.1772, -0.9795,  0.6968,  ...,  0.8882,  0.4773,  0.6655],\n",
      "          [-0.9092, -0.7246,  0.7646,  ...,  0.5947, -0.1184, -0.1648],\n",
      "          ...,\n",
      "          [ 0.0610, -0.1594,  1.2842,  ...,  0.7402,  0.9658,  0.4412],\n",
      "          [-0.1548, -0.8047,  0.2896,  ...,  0.7363,  0.3633,  0.4548],\n",
      "          [ 0.6406, -0.7051,  0.8906,  ...,  0.5459,  0.0742,  0.7563]],\n",
      "\n",
      "         [[ 0.0290, -1.0439,  0.7324,  ...,  0.6445,  0.6006,  0.5669],\n",
      "          [ 0.2114, -1.1240,  1.0088,  ...,  0.8643,  0.7861,  0.7725],\n",
      "          [-0.7124, -1.0107,  1.0430,  ...,  0.7812, -0.1003, -0.1357],\n",
      "          ...,\n",
      "          [ 0.0952, -0.4875,  0.8730,  ...,  0.6768,  0.6577,  0.4163],\n",
      "          [ 0.4424, -0.0840,  1.5137,  ...,  0.3701,  0.5679, -0.0168],\n",
      "          [ 1.0195, -0.6099,  1.1738,  ...,  0.8711,  0.1316,  0.7393]],\n",
      "\n",
      "         [[ 0.2397, -1.1797,  0.6484,  ...,  0.5596,  0.2710,  0.6245],\n",
      "          [ 0.1917, -1.0869,  0.8003,  ...,  0.9653,  0.4497,  0.6128],\n",
      "          [-0.6133, -0.8433,  0.7358,  ...,  0.4783, -0.2095, -0.3320],\n",
      "          ...,\n",
      "          [-0.1240, -0.4607,  0.6699,  ...,  1.0332,  0.8706,  0.8818],\n",
      "          [ 0.2058, -0.5254,  1.2070,  ...,  0.6768,  0.4041,  0.2881],\n",
      "          [ 0.5244, -0.5098,  1.0557,  ...,  0.4146, -0.1304,  0.8062]]]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "in encode_videos.mm_projector video_features= tensor([[[[ 1.3330,  0.8701, -0.8965,  ...,  1.0244, -0.3298,  1.6279],\n",
      "          [ 1.9395,  1.1699, -0.2437,  ...,  1.2080,  0.2629,  1.3623],\n",
      "          [ 1.6660,  1.6387, -0.7583,  ...,  0.7178,  0.5923,  1.2334],\n",
      "          ...,\n",
      "          [ 1.4873,  1.7480, -0.6348,  ...,  1.3916,  0.2949,  0.6372],\n",
      "          [ 0.5620,  0.9131, -0.1802,  ...,  0.8364, -0.1857,  0.6675],\n",
      "          [ 1.8936,  1.1826, -0.1736,  ...,  1.2559,  0.3750,  1.5088]],\n",
      "\n",
      "         [[ 1.1113,  1.2139, -1.0674,  ...,  0.3958, -0.4932,  1.6885],\n",
      "          [ 1.4209,  1.0244,  0.2717,  ...,  0.8086,  0.0772,  1.0010],\n",
      "          [ 1.6182,  1.6426, -0.7993,  ...,  0.3635,  0.3235,  0.7036],\n",
      "          ...,\n",
      "          [ 1.1621,  1.1748, -0.4441,  ...,  1.3486,  0.0974,  0.6211],\n",
      "          [ 1.3799,  0.4795, -0.0828,  ...,  0.7847,  0.9087,  1.5752],\n",
      "          [ 1.8262,  1.2959, -0.3284,  ...,  1.0371,  0.3979,  1.4512]],\n",
      "\n",
      "         [[ 1.2363,  1.0508, -0.9326,  ...,  0.6006, -0.3784,  2.1055],\n",
      "          [ 1.8301,  1.1992,  0.0365,  ...,  1.2822,  0.1320,  1.2783],\n",
      "          [ 1.5625,  1.5732, -0.6992,  ...,  0.9648,  0.4124,  1.2070],\n",
      "          ...,\n",
      "          [ 1.7285,  1.0713, -0.4895,  ...,  1.7705,  0.2612,  1.2354],\n",
      "          [ 0.7632,  1.0195, -0.4424,  ...,  1.0576, -0.1865,  0.9897],\n",
      "          [ 2.0098,  0.7983, -0.3003,  ...,  1.2861,  0.4871,  1.6650]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.0586,  1.1084, -0.6646,  ...,  0.6167, -0.3621,  1.3721],\n",
      "          [ 1.7100,  0.7397,  0.1940,  ...,  1.0332,  0.2935,  1.2764],\n",
      "          [ 1.6084,  1.3994, -0.2317,  ...,  0.8286,  0.6230,  0.9673],\n",
      "          ...,\n",
      "          [ 1.2031,  0.7993,  0.3806,  ...,  1.3428, -0.3447,  0.7788],\n",
      "          [ 1.5332,  1.1670, -0.4287,  ...,  1.5459,  0.2551,  1.3408],\n",
      "          [ 1.7832,  1.0381,  0.1245,  ...,  1.5459,  0.3149,  1.2568]],\n",
      "\n",
      "         [[ 1.2998,  0.7944, -0.4041,  ...,  0.8931, -0.5396,  1.2578],\n",
      "          [ 1.5127,  1.1797, -0.0715,  ...,  1.0088,  0.4297,  0.8857],\n",
      "          [ 1.7305,  1.2021, -0.5029,  ...,  1.0283,  0.7207,  1.2959],\n",
      "          ...,\n",
      "          [ 1.3008,  0.8755, -0.2227,  ...,  1.4814, -0.0147,  0.8701],\n",
      "          [ 0.7100,  0.4902, -0.0357,  ...,  0.9238, -0.3376,  0.6577],\n",
      "          [ 1.1113,  0.6104, -0.3845,  ...,  1.8984,  0.4753,  1.2666]],\n",
      "\n",
      "         [[ 1.1816,  1.0195, -0.4478,  ...,  0.4590, -0.7217,  1.7705],\n",
      "          [ 1.7197,  1.3252, -0.3621,  ...,  0.8501,  0.3069,  1.1484],\n",
      "          [ 1.9219,  1.2109, -0.6548,  ...,  0.7397,  0.4856,  1.2578],\n",
      "          ...,\n",
      "          [ 1.6855,  1.2754, -0.4153,  ...,  1.2939,  0.2219,  0.7500],\n",
      "          [ 1.5225,  1.0752, -0.2032,  ...,  1.5977,  0.0221,  1.0635],\n",
      "          [ 1.4492,  1.1152, -0.0394,  ...,  1.1152,  0.1404,  1.7061]]]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "LLaVA 回答： The\n"
     ]
    }
   ],
   "source": [
    "num_image_tokens = (input_ids == IMAGE_TOKEN_INDEX).sum().item()\n",
    "#print(\"你 prompt 中的 <image> token 數量 =\", num_image_tokens)\n",
    "#print(projected.shape)\n",
    "#print(list[projected])\n",
    "with torch.inference_mode():\n",
    "    output_ids = LLaVA_model.generate(\n",
    "        input_ids=input_ids,\n",
    "        images=pixel_values,  \n",
    "        do_sample=True,\n",
    "        temperature=0.1,\n",
    "        max_new_tokens=1024,\n",
    "        use_cache=True,\n",
    "        stopping_criteria=[stopping_criteria],\n",
    "    )\n",
    "\n",
    "output_text = tokenizer.decode(output_ids[0, input_ids.shape[1]:], skip_special_tokens=True)\n",
    "print(\"LLaVA 回答：\", output_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "videollava",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
