import av
import torch
import numpy as np
from PIL import Image
from transformers import AutoProcessor, AutoModel
import torch.nn.functional as F
import types
np.random.seed(0)
from torchvision import transforms

from videollava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN
from videollava.conversation import conv_templates, SeparatorStyle
from videollava.model.builder import load_pretrained_model
from videollava.utils import disable_torch_init
from videollava.mm_utils import tokenizer_image_token, get_model_name_from_path, KeywordsStoppingCriteria




def main():
    np.random.seed(0)
    ###
    def read_video_pyav(path, num_frames=16):
        '''
        Decode the video with PyAV and return a list of PIL.Image.
        '''
        container = av.open(path)
        stream = container.streams.video[0]
        total_frames = stream.frames or 100  # fallback if frame count is unknown
        indices = np.linspace(0, total_frames - 1, num_frames).astype(int)

        frames = []
        for i, frame in enumerate(container.decode(video=0)):
            if i in indices:
                frames.append(frame.to_image())  # Convert to PIL.Image
            if len(frames) >= num_frames:
                break
        return frames
    # ğŸŸ© æ›¿æ›ç‚ºä½ çš„æœ¬åœ°å½±ç‰‡è·¯å¾‘
    video_path = "/my_workspace/Data/videos/00001.mp4"
    frames = read_video_pyav(video_path)
    # ğŸŸ¦ æ›¿æ›ç‚ºä½ çš„æ–°èå¥å­
    text = [
        "é—œå¿ƒæ˜¨å¤©æ™šé–“6é»36åˆ†ï¼ŒèŠ±è“®å¯Œé‡Œé„‰ã€ç™¼ç”ŸèŠ®æ°è¦æ¨¡5.8åœ°éœ‡ï¼Œå¹¾ä¹å…¨å°æœ‰æ„Ÿï¼Œæ‰€å¹¸éƒ½æ²’æœ‰å‚³å‡ºåš´é‡ç½æƒ…ï¼Œæ°£è±¡ç½²åœ°éœ‡æ¸¬å ±ä¸­å¿ƒä¸æ’é™¤ä»Šæ˜å¾Œä¸‰å¤©ï¼Œé‚„å¯èƒ½æœƒæœ‰è¦æ¨¡4.5åˆ°5.5çš„é¤˜éœ‡ï¼Œå¤§å®¶å¤šåŠ ç•™æ„ï¼ƒã€‚",
        "é—œå¿ƒæ—©ä¸Šåœ°ç‰›ç¿»èº«ï¼Œ07:05èŠ±è“®å¤–æµ·ç™¼ç”Ÿè¦æ¨¡6.2çš„æœ‰æ„Ÿåœ°éœ‡ï¼Œéœ‡åº¦3ç´šä»¥ä¸Šåœ°å€ï¼ŒåŒ…æ‹¬æœ‰å®œè˜­ã€èŠ±è“®ã€æ–°åŒ—ã€æ–°ç«¹ã€å˜‰ç¾©ã€é›²æ—ã€å½°åŒ–ã€‚æ›´å¤šåœ°éœ‡æ¶ˆæ¯ï¼Œè«‹é–å®šå…¬è¦–å„ç¯€æ–°èã€‚",
        "å˜‰ç¾©ç¸£æ–°æ¸¯é„‰æ˜¨å¤©ä¸‹åˆäº”é»åŠï¼Œç™¼ç”ŸèŠ®æ°è¦æ¨¡5.5åœ°éœ‡ï¼Œæ·±åº¦8.5å…¬é‡Œï¼Œåœ°éœ‡æ˜é¡¯ï¼Œä½¿å¾—é«˜éµå…©ç­åˆ—è»Šæš«æ™‚åœè»Šï¼Œå˜‰ç¾©æ°‘é›„é„‰æœ‰æ°‘å®…åœç‰†è¢«éœ‡å€’ï¼Œé‚„æœ‰é›²æ—å¤å‘é„‰149ç”²ç·šå…§æ¹–æ˜éš§é“å‰‡ç™¼ç”Ÿåæ–¹è½çŸ³ï¼Œæ‰€å¹¸ç„¡äººå—å‚·ã€‚",
        "å…”å¹´æ˜¥ç¯€10å¤©é€£å‡å·²ç¶“çµæŸï¼Œä»Šå¤©é–‹å§‹ä¸Šç­ï¼Œä¸å°‘äººæ—©ä¸Šèµ·ä¸ä¾†ï¼Œä»Šå¹´éå¹´å†·é¢¼é¢¼ï¼Œå¯’æµç™¼å¨ï¼Œæ˜¨å¤©æ¸…æ™¨è‹—æ —é ­å±‹å‡ºç¾4åº¦ä½æº«ï¼Œæ˜¯å…¥å†¬ä»¥ä¾†æœ€ä½æ°£æº«ç´€éŒ„ã€‚å…‰æ˜¯1æœˆ27æ—¥åˆ°1æœˆ28æ—¥äºŒå¤©ï¼Œå…¨å°è¶…é140äººï¼Œç–‘ä¼¼å› å¤©å†·çŒæ­»ã€‚æ°£è±¡å±€æŒ‡ç¤ºï¼Œä»Šå¤©å¯’æµé›–ç„¶é–‹å§‹æ¸›å¼±ï¼Œä¸éï¼Œæ˜å¾Œå¤©ï¼Œå„åœ°éƒ½é‚„æ˜¯æ—¥å¤œæº«å·®å¤§ï¼Œå¤§å®¶å‡ºé–€è¦åšå¥½ä¿æš–å·¥ä½œã€‚",
        "é—œå¿ƒåœ°éœ‡æ¶ˆæ¯", 
        "ä»Šå¤©æ™šé¤åƒä»€éº¼", 
        "æ­¡è¿æ”¶çœ‹å…¬è¦–æ–°è"]
    # ğŸŸ¨ è¼‰å…¥å¤šæ¨¡æ…‹ XCLIP æ¨¡å‹ï¼ˆlarge ç‰ˆæœ¬æ”¯æ´ vision-textï¼‰
    processor = AutoProcessor.from_pretrained("microsoft/xclip-large-patch14-16-frames")
    XCLIP_model = AutoModel.from_pretrained("microsoft/xclip-large-patch14-16-frames").to("cuda").eval()
    
    ### å®šé¸è£¡é¢çš„æ±è¥¿
    def hook_visual_proj_input(module, input, output):
        print(f"[ğŸ¯ visual_projection input] shape = {input[0].shape}")
        module._cached_input = input[0].detach().cpu()

    handle = XCLIP_model.visual_projection.register_forward_hook(hook_visual_proj_input)
    ###
    video_inputs = processor.image_processor(
        images=[frames],
        return_tensors="pt",
        size={"height": 336, "width": 336}, 
        do_resize=True,
        do_center_crop=False  
    )
    text_inputs = processor.tokenizer(
        text,
        return_tensors="pt",
        padding=True,
        truncation=True
    )
    '''
    print(video_inputs["pixel_values"].shape)
    print("---")
    print(text_inputs["input_ids"].shape)
    '''
    inputs = {
        "pixel_values": video_inputs["pixel_values"].to("cuda"),
        "input_ids": text_inputs["input_ids"].to("cuda"),
        "attention_mask": text_inputs["attention_mask"].to("cuda")
    }
    # å‰å‘æ¨è«–
    with torch.no_grad():
        outputs = XCLIP_model(**inputs)

    # é€™è¡Œæ›¿ä»£åŸæœ¬çš„ outputs.video_embeds
    video_tensor = XCLIP_model.visual_projection._cached_input.to(dtype=torch.float16).to(XCLIP_model.device)  # [1, 16, 768]


    print("XCLIPçµæŸ")

    print("LLaVAé–‹å§‹")
    disable_torch_init()
    inp = 'This is a sign language video. Please translate it into English.'
    '''
    video = 'videollava/serve/examples/sample_demo_1.mp4'
    model_path = 'LanguageBind/Video-LLaVA-7B'
    cache_dir = 'cache_dir'
    device = 'cuda'
    load_4bit, load_8bit = True, False
    '''
    # âœ… æ”¹ç‚ºæœ¬åœ°æ¨¡å‹è·¯å¾‘
    model_path = '/my_workspace/Video-LLaVA-7B'
    cache_dir = 'cache_dir'  # âœ… æœ¬åœ°æ¨¡å‹ä¸ç”¨è¨­å®š cache_dir
    device = 'cuda'
    load_4bit, load_8bit = True, False  # å»ºè­°æœ¬åœ°å…ˆç”¨ fp16ï¼Œç©©å®šæ€§è¼ƒé«˜
    model_name = 'Video-LLaVA-7B'
    print("LLaVAè¼‰å…¥æ¨¡å‹") 
    tokenizer, LLaVA_model, processor, _ = load_pretrained_model(model_path, None, model_name, load_8bit, load_4bit, device=device, cache_dir=cache_dir)
    print("LLaVAçµæŸè¼‰å…¥")
    #
    #video_processor = processor['video']
    #
    conv_mode = "llava_v1"
    conv = conv_templates[conv_mode].copy()
    roles = conv.roles
    #
    #print(LLaVA_model)
    #print(type(LLaVA_model))
    #print(LLaVA_model.model)
    #print(type(LLaVA_model.model))
    
    # é€šé LLaVA çš„æŠ•å½±å±¤è½‰æˆ [1, 16, 4096]
    video_tensor = LLaVA_model.model.mm_projector(video_tensor).unsqueeze(0)

    print("tensor.shape =",video_tensor.shape)
    
    print(f"{roles[1]}: {inp}")
    num_tokens = video_tensor.shape[1]
    inp = ' '.join([DEFAULT_IMAGE_TOKEN] * num_tokens) + '\n' + inp

    conv.append_message(conv.roles[0], inp)
    conv.append_message(conv.roles[1], None)
    prompt = conv.get_prompt()
    input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).cuda()
    stop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2
    keywords = [stop_str]
    stopping_criteria = KeywordsStoppingCriteria(keywords, tokenizer, input_ids)
    print("input_ids.shape + video_tensor.shape",input_ids.shape,video_tensor.shape)
    with torch.inference_mode():
        output_ids = LLaVA_model.generate(
            input_ids,
            inputs_embeds=video_tensor,  # âœ… æ”¹é€™è¡Œ
            do_sample=True,
            temperature=0.1,
            max_new_tokens=1024,
            use_cache=True,
            stopping_criteria=[stopping_criteria])

    print("LLaVA outputs ç”¢ç”Ÿ")
    outputs = tokenizer.decode(output_ids[0, input_ids.shape[1]:]).strip()
    print("LLaVA outputs çµæŸ")
    print("Final output",outputs)

if __name__ == '__main__':
    main()